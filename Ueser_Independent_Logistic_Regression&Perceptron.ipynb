{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yzhao07/MLMA_EOG/blob/main/Ueser_Independent_Logistic_Regression%26Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMEjTi0Ag1Sx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pickle\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W__iF6fHhC4N",
        "outputId": "05aca56e-9866-49b5-fbf9-1c924dfc59c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_dir = '/content/drive/MyDrive/S1Database/isolated/'\n",
        "os.listdir(main_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo3csk7aiimC",
        "outputId": "7418c4d0-dac5-4542-bc05-94b119e3c776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ReadMe', '004', '002', '006', '003', '005', '001']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get data\n",
        "def get_original_feature_label(path):\n",
        "    # scale both vertical and horizontal between 0 and 1\n",
        "    scaler = MinMaxScaler()\n",
        "    feature = []\n",
        "    label = []\n",
        "    for i in os.listdir(path):\n",
        "        a = pd.read_csv(str(path+i),names=[\"vertical\",\"horizontal\"])\n",
        "        a = np.array(a)\n",
        "        a = scaler.fit_transform(a)\n",
        "        a = np.swapaxes(a,0,1)\n",
        "        feature.append(a)\n",
        "        if i.endswith(\"001.csv\"):\n",
        "            label.append(0)\n",
        "        elif i.endswith(\"002.csv\"):\n",
        "            label.append(1)\n",
        "        elif i.endswith(\"003.csv\"):\n",
        "            label.append(2)\n",
        "        elif i.endswith(\"004.csv\"):\n",
        "            label.append(3)\n",
        "        elif i.endswith(\"005.csv\"):\n",
        "            label.append(4)\n",
        "        elif i.endswith(\"006.csv\"):\n",
        "            label.append(5)\n",
        "        elif i.endswith(\"007.csv\"):\n",
        "            label.append(6)\n",
        "        elif i.endswith(\"008.csv\"):\n",
        "            label.append(7)\n",
        "        elif i.endswith(\"009.csv\"):\n",
        "            label.append(8)\n",
        "        elif i.endswith(\"010.csv\"):\n",
        "            label.append(9)\n",
        "        elif i.endswith(\"011.csv\"):\n",
        "            label.append(10)\n",
        "        elif i.endswith(\"012.csv\"):\n",
        "            label.append(11)\n",
        "    \n",
        "    return feature,label"
      ],
      "metadata": {
        "id": "gckJ96syiuEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom dataset class\n",
        "class MYDataset(Dataset):\n",
        "    def __init__(self, feature,label):\n",
        "      self.feature = []\n",
        "      self.label = []\n",
        "      for i in range(len(feature)):\n",
        "        self.feature.append(feature[i])\n",
        "        self.label.append(label[i])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        f = self.feature[idx]\n",
        "        f = f[np.newaxis,:,:]\n",
        "        l = self.label[idx]\n",
        "        return f, l\n"
      ],
      "metadata": {
        "id": "sHLZfBH7jDuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split on patient level - User-independent"
      ],
      "metadata": {
        "id": "48eGmXRonF8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patient = [\"001\",'002','003','004','005']\n",
        "train_feature = []\n",
        "train_label = []\n",
        "for i in patient:\n",
        "    f,l = get_original_feature_label(str(main_dir+i+\"/isolated_strokes/\"))\n",
        "    #print(len(f),f[0].shape)\n",
        "    for x in f:\n",
        "        train_feature.append(x)         \n",
        "    for x in l:\n",
        "        train_label.append(x)\n",
        "print(len(train_feature))\n",
        "train_data = MYDataset(train_feature,train_label)\n",
        "train_dataloader  = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "\n",
        "\n",
        "patient = [\"006\"]\n",
        "test_feature = []\n",
        "test_label = []\n",
        "for i in patient:\n",
        "    f,l = get_original_feature_label(str(main_dir+i+\"/isolated_strokes/\"))\n",
        "    #print(len(f),f[0].shape)\n",
        "    for x in f:\n",
        "        test_feature.append(x)         \n",
        "    for x in l:\n",
        "        test_label.append(x)\n",
        "print(len(test_feature))\n",
        "test_data = MYDataset(test_feature,test_label)\n",
        "test_dataloader  = DataLoader(test_data, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRo6eTpmkHux",
        "outputId": "479dfc67-a8a3-4a48-b6f7-eb4fdfdb432c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "603\n",
            "121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, f1_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "# Set input and output dimensions\n",
        "input_dim = 2 * 1250\n",
        "output_dim = 12\n",
        "\n",
        "# Flatten the input features\n",
        "train_feature_flat = [x.flatten() for x in train_feature]\n",
        "test_feature_flat = [x.flatten() for x in test_feature]\n",
        "\n",
        "# Create a logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_feature_flat, train_label)\n",
        "\n",
        "# Make predictions on test data\n",
        "test_predictions = model.predict(test_feature_flat)\n",
        "\n",
        "# Get predicted probabilities\n",
        "test_probabilities = model.predict_proba(test_feature_flat)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_label, test_predictions)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion = confusion_matrix(test_label, test_predictions)\n",
        "\n",
        "# Binarize the labels and predicted probabilities\n",
        "test_label_binary = label_binarize(test_label, classes=np.arange(output_dim))\n",
        "test_probabilities_binary = test_probabilities\n",
        "\n",
        "# Calculate AUC for each class\n",
        "auc = roc_auc_score(test_label_binary, test_probabilities_binary, multi_class='ovr')\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(test_label, test_predictions, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\", confusion)\n",
        "print(\"AUC:\", auc)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCRLjlbLjhDi",
        "outputId": "7d4f353a-3ac9-483e-bd83-69eca43d7a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7851239669421488\n",
            "Confusion Matrix: [[ 4  0  0  0  4  1  0  0  0  1  0  0]\n",
            " [ 0  4  0  6  0  0  0  0  0  0  0  0]\n",
            " [ 0  0 10  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  9  0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0 11  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0 10  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0 10  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 10  0  0  0  0]\n",
            " [ 5  0  0  3  0  0  0  0  2  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 10  0  0]\n",
            " [ 0  0  0  0  0  0  3  0  0  0  7  0]\n",
            " [ 0  2  0  0  0  0  0  0  0  0  0  8]]\n",
            "AUC: 0.9835207107934382\n",
            "F1 Score: 0.7676955979921019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# Set input and output dimensions\n",
        "input_dim = 2 * 1250\n",
        "output_dim = 12\n",
        "\n",
        "# Flatten the input features\n",
        "train_feature_flat = [x.flatten() for x in train_feature]\n",
        "test_feature_flat = [x.flatten() for x in test_feature]\n",
        "\n",
        "# Create a perceptron model\n",
        "model = Perceptron()\n",
        "\n",
        "# Calibrate the classifier to get predicted probabilities\n",
        "calibrated_model = CalibratedClassifierCV(model, method='sigmoid', cv=5)\n",
        "calibrated_model.fit(train_feature_flat, train_label)\n",
        "\n",
        "# Make predictions on test data\n",
        "test_predictions = calibrated_model.predict(test_feature_flat)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_label, test_predictions)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion = confusion_matrix(test_label, test_predictions)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(test_label, test_predictions, average='weighted')\n",
        "\n",
        "# Calculate probabilities for AUC\n",
        "test_probabilities = calibrated_model.predict_proba(test_feature_flat)\n",
        "\n",
        "# Binarize the test labels for multiclass AUC calculation\n",
        "test_label_binarized = label_binarize(test_label, classes=np.arange(output_dim))\n",
        "\n",
        "# Calculate AUC\n",
        "auc = roc_auc_score(test_label_binarized, test_probabilities, multi_class='ovo', average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\", confusion)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"AUC:\", auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMm_1RidF70n",
        "outputId": "8047a084-8db7-431f-fbdc-a02b968b8b57"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6859504132231405\n",
            "Confusion Matrix: [[ 1  0  0  0  6  1  1  0  0  1  0  0]\n",
            " [ 0  1  0  6  0  2  0  1  0  0  0  0]\n",
            " [ 0  0 10  0  0  0  0  0  0  0  0  0]\n",
            " [ 1  0  0  8  0  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0 11  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0 10  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0 10  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 10  0  0  0  0]\n",
            " [ 7  0  0  2  0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 10  0  0]\n",
            " [ 0  0  0  0  0  0  8  0  0  0  2  0]\n",
            " [ 0  0  0  0  0  1  0  0  0  0  0  9]]\n",
            "F1 Score: 0.6300569159308341\n",
            "AUC: 0.9582296045105962\n"
          ]
        }
      ]
    }
  ]
}
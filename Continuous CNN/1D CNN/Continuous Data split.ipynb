{"cells":[{"cell_type":"markdown","metadata":{"id":"bGJOJ1vZS_HF"},"source":["# import "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17025,"status":"ok","timestamp":1682434012795,"user":{"displayName":"戴劲","userId":"11036111496252500300"},"user_tz":240},"id":"1ZbMK-khTIsd","outputId":"fd9bc2df-40b8-4594-c511-80b874bfea1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1682434205111,"user":{"displayName":"戴劲","userId":"11036111496252500300"},"user_tz":240},"id":"78JZVMvWT1Nl"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/MLMA/project/Code Continuous Data')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":862,"status":"ok","timestamp":1682434208321,"user":{"displayName":"戴劲","userId":"11036111496252500300"},"user_tz":240},"id":"OaSlW2eobWLs"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","from stroke_probability_map import *"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":594,"status":"ok","timestamp":1682434208912,"user":{"displayName":"戴劲","userId":"11036111496252500300"},"user_tz":240},"id":"_cWpoQ2uS_HH","outputId":"19aaa008-efe4-4e76-e609-201baa0e9d4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2\n","from stroke_probability_map_1 import *"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4371,"status":"ok","timestamp":1682434213281,"user":{"displayName":"戴劲","userId":"11036111496252500300"},"user_tz":240},"id":"stgVSD3US_HI"},"outputs":[],"source":["import numpy as np\n","import os\n","import math\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","\n","import pandas as pd\n","\n","from scipy import signal\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils import data as Data\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim.lr_scheduler as lr_scheduler\n","\n","from tqdm import tqdm\n","\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"INdrL3EhQ0wf"},"source":["# File and Path"]},{"cell_type":"markdown","metadata":{"id":"VR4iBbuGS_HI"},"source":["File name:   \n","EOG_00x_xx_xxx.csv:   \n"," 00x: participant's ID.    \n"," xx:  index of the recording time.    \n"," xxx: index of the corresponding eye movements.    \n","      - e.g., xxx is the index of a training word.   "]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682434213281,"user":{"displayName":"戴劲","userId":"11036111496252500300"},"user_tz":240},"id":"9EIba5Pg2Zcq"},"outputs":[],"source":["result_path = '/content/drive/MyDrive/Colab Notebooks/MLMA/project/Code Continuous Data/Result'"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1682434213546,"user":{"displayName":"戴劲","userId":"11036111496252500300"},"user_tz":240},"id":"qieqdMYZX17L"},"outputs":[],"source":["data_path = '/content/drive/MyDrive/Colab Notebooks/MLMA/project/Data/continuous'"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682434213546,"user":{"displayName":"戴劲","userId":"11036111496252500300"},"user_tz":240},"id":"KWC2DhqhrkeI"},"outputs":[],"source":["result_path_isolated = '/content/drive/MyDrive/Colab Notebooks/MLMA/project/Code Isolated Data/Result'"]},{"cell_type":"markdown","metadata":{"id":"BXaoEpn8PfPG"},"source":["# Dataset and Dataloader"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":172,"status":"ok","timestamp":1682434216032,"user":{"displayName":"戴劲","userId":"11036111496252500300"},"user_tz":240},"id":"WssXMkpEPi7a"},"outputs":[],"source":["class MyDataset(Dataset):\n","  def __init__(self, ID_list, R_time_list, model_list, flatten, model_type, root_dir='/content/drive/MyDrive/Colab Notebooks/MLMA/project/Data/continuous/00'):\n","    self.data = []\n","    self.stroke_probability_map = []\n","    self.label = np.zeros((0,))\n","    for id_idx in range(len(ID_list)):\n","      file_list = os.listdir(root_dir + str(ID_list[id_idx]) +'/training/')\n","      for f_idx in range(len(file_list)):\n","        if int(file_list[f_idx][8:10]) in R_time_list:\n","          data = pd.read_csv(root_dir + str(ID_list[id_idx]) +'/training/' + file_list[f_idx],  header=None).to_numpy()\n","          #print(data.shape)\n","          data = signal.resample(data, 1024, axis=0)\n","          #print(data.shape)\n","          s = stroke_probability_map_1(data, model_list[id_idx], flatten, model_type)\n","          s = torch.from_numpy(s)\n","          self.stroke_probability_map.append(s)\n","          self.label = np.append(self.label, np.ones((1,))*(int(file_list[f_idx][-7:-4])-1), axis=0)\n","          data = (data - data.mean(axis=0, keepdims=True))/np.std(data,axis=0, keepdims=True)\n","          data = torch.from_numpy(data)\n","          self.data.append(data)\n","\n","    self.label = torch.from_numpy(self.label)\n","  def __getitem__(self, index) :\n","    return self.data[index].float(), self.stroke_probability_map[index].float(), self.label[index].type(torch.LongTensor)\n","  def __len__(self):\n","    return len(self.data)"]},{"cell_type":"markdown","metadata":{"id":"78A6FXCrT_v0"},"source":["# Model"]},{"cell_type":"code","source":["# 比1复杂\n","class My_CNN_2_split_B(nn.Module):\n","  def __init__(self, N=16):\n","    super(My_CNN_2_split_B, self).__init__()\n","    self.conv1 = nn.Conv1d(in_channels=2, out_channels=16, kernel_size=9, padding=5)\n","    nn.init.xavier_uniform_(self.conv1.weight)\n","    nn.init.constant_(self.conv1.bias, 0.0)\n","    self.mp1 = nn.MaxPool1d(kernel_size=4)\n","    self.relu1 = nn.LeakyReLU()\n","\n","    self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=3)\n","    nn.init.xavier_uniform_(self.conv2.weight)\n","    nn.init.constant_(self.conv2.bias, 0.0)\n","    self.mp2 = nn.MaxPool1d(kernel_size=2)\n","    self.relu2 = nn.LeakyReLU()\n","\n","    self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=2)\n","    nn.init.xavier_uniform_(self.conv3.weight)\n","    nn.init.constant_(self.conv3.bias, 0.0)\n","    self.mp3 = nn.MaxPool1d(kernel_size=2)\n","    self.relu3 = nn.LeakyReLU()\n","\n","    self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=2)\n","    nn.init.xavier_uniform_(self.conv4.weight)\n","    nn.init.constant_(self.conv4.bias, 0.0)\n","    self.amp =  nn.AdaptiveMaxPool1d(N)\n","    self.sigmoid = nn.Sigmoid()\n","    \n","\n","    self.fc1 = nn.Linear(int(128), 25)\n","    nn.init.xavier_uniform_(self.fc1.weight)\n","    nn.init.constant_(self.fc1.bias, 0.0)\n","    self.relu5 = nn.LeakyReLU()\n","\n","    self.flatten = nn.Flatten()\n","    self.dropout = nn.Dropout(p=0.5)\n","\n","    self.fc2 = nn.Linear(25*16, 150)\n","    nn.init.xavier_uniform_(self.fc2.weight)\n","    nn.init.constant_(self.fc2.bias, 0.0)\n","\n","  def forward(self, x, probability):\n","    x = torch.transpose(x, -1, -2)\n","\n","    x = self.relu1(self.mp1(self.conv1(x)))\n","    x = self.relu2(self.mp2(self.conv2(x)))\n","    x = self.relu3(self.mp3(self.conv3(x)))\n","    x = self.conv4(x)\n","    x = self.sigmoid(self.amp(x))\n","    x = torch.transpose(x, -1, -2)\n","\n","    x = self.relu5(self.fc1(x))\n","    x = self.flatten(x)\n","    x = self.dropout(x)\n","    x = self.fc2(x)\n","\n","    return x"],"metadata":{"id":"yaioigQh4d2Y","executionInfo":{"status":"ok","timestamp":1682434984588,"user_tz":240,"elapsed":1,"user":{"displayName":"戴劲","userId":"11036111496252500300"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# 比1复杂\n","class My_CNN_2_split_A(nn.Module):\n","  def __init__(self, N=16):\n","    super(My_CNN_2_split_A, self).__init__()\n","    self.fc1 = nn.Linear(int(3*12), 25)\n","    nn.init.xavier_uniform_(self.fc1.weight)\n","    nn.init.constant_(self.fc1.bias, 0.0)\n","    self.relu5 = nn.LeakyReLU()\n","\n","    self.flatten = nn.Flatten()\n","    self.dropout = nn.Dropout(p=0.5)\n","\n","    self.fc2 = nn.Linear(25*16, 150)\n","    nn.init.xavier_uniform_(self.fc2.weight)\n","    nn.init.constant_(self.fc2.bias, 0.0)\n","\n","  def forward(self, x, probability):\n","    x = probability\n","    x = self.relu5(self.fc1(x))\n","    x = self.flatten(x)\n","    x = self.dropout(x)\n","    x = self.fc2(x)\n","\n","    return x"],"metadata":{"id":"m08jdMeST4FB","executionInfo":{"status":"ok","timestamp":1682434974933,"user_tz":240,"elapsed":3,"user":{"displayName":"戴劲","userId":"11036111496252500300"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HVydt1AQo0Fs"},"source":["# Some Funtions"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":122,"status":"ok","timestamp":1682434226647,"user":{"displayName":"戴劲","userId":"11036111496252500300"},"user_tz":240},"id":"2w7ojQc8o-mm"},"outputs":[],"source":["def Train_model(model,epochs,optmizer,criterion, train_loader):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.train()\n","    train_loss = []\n","    x = 0\n","    for batch_idx, (data, probability, label) in enumerate(train_loader):\n","        data = data.to(device)\n","        label = label.to(device)\n","        probability = probability.to(device)\n","        optmizer.zero_grad()  \n","        output = model(data, probability)\n","        # print(type(output), type(label))\n","        loss = criterion(output, label)\n","        loss.backward()\n","        optmizer.step() \n","        train_loss.append(loss.item()) \n","        #if(batch_idx+1)%3 == 0:\n","            #print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.\\\n","                #format(epochs, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\n","    return np.mean(train_loss), model"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1682434229102,"user":{"displayName":"戴劲","userId":"11036111496252500300"},"user_tz":240},"id":"ozKFJi50pKec"},"outputs":[],"source":["def test(model,epochs,criterion, dataloader, T_T='Test'):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.eval()\n","    test_loss = 0\n","    c = 0\n","    y_pred = np.zeros((0,))\n","    y_true = np.zeros((0, ))\n","    with torch.no_grad():\n","        for data, probability, label in dataloader:\n","            data = data.to(device)\n","            label = label.to(device)\n","            probability = probability.to(device)\n","            output = model(data, probability)\n","            test_loss += criterion(output, label).item()\n","            y_pred = np.append(y_pred, output.cpu().argmax(dim=-1), axis=0)\n","            y_true = np.append(y_true, label.cpu(), axis=0)\n","            c += 1\n","    test_loss /= c\n","    #if epochs%50 == 0:\n","        #print('EPOCH:{}    {} set: Average loss: {:.4f}'.format(epochs, T_T, test_loss))\n","  \n","    return test_loss, accuracy_score(y_true, y_pred), f1_score(y_true, y_pred, average='micro', zero_division=0), \\\n","    f1_score(y_true, y_pred, average='macro', zero_division=0), precision_score(y_true, y_pred, average='micro', zero_division=0), \\\n","    precision_score(y_true, y_pred, average='macro', zero_division=0), recall_score(y_true, y_pred, average='micro', zero_division=0), \\\n","    recall_score(y_true, y_pred, average='macro', zero_division=0)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":356,"status":"ok","timestamp":1682434831901,"user":{"displayName":"戴劲","userId":"11036111496252500300"},"user_tz":240},"id":"wW7yqn0wpRYf"},"outputs":[],"source":["def main_function(total_epoch=100, dataloader_list=None, model_para=None, model_save=False, model_name=None, cross_val=True, weight_decay=1e-3):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    \n","    if model_para==0:\n","      model = My_CNN_2_split_A().to(device)\n","    else:\n","      model = My_CNN_2_split_B().to(device)\n","        \n","    optmizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=weight_decay)  \n","    #optmizer = optim.SGD(model.parameters(), lr=1e-3, weight_decay=weight_decay) \n","    scheduler = lr_scheduler.StepLR(optmizer, step_size=50, gamma=0.5, verbose=False) \n","    criterion = nn.CrossEntropyLoss().to(device)  \n","    \n","    train_loss = np.zeros((total_epoch))\n","    score = np.zeros((len(dataloader_list), 8, total_epoch)) # loss, accuracy, f1 score*2, precision*2, recall*2\n","    \n","    for i in tqdm(range(total_epoch)):\n","        train_loss[i], model = Train_model(model, i, optmizer, criterion, dataloader_list[0])\n","        scheduler.step()\n","        for j in range(len(dataloader_list)):\n","            if j == 0:\n","                T_T = 'Train'\n","            elif j == 1:\n","                T_T = 'Validation'\n","            else:\n","                T_T = 'Test'\n","            score[j, :, i] = test(model, i, criterion, dataloader_list[j], T_T=T_T)\n","        #if np.argmin(score[1, 0, :(i+1)]) == i:\n","            #if model_save:\n","                #torch.save(model.state_dict(), model_name)     \n","    #if model_save: \n","        #np.savez(model_name[:-4], train_loss=train_loss, score=score)\n","    if model_save:\n","      torch.save(model.state_dict(), model_name)\n","    if cross_val:\n","        epoch_best = np.argmin(score[1, 0, :])\n","        return score[:, :, epoch_best]\n","    else:\n","        return train_loss, score"]},{"cell_type":"markdown","source":["## Leave one trial out split"],"metadata":{"id":"dFms6R6clqEQ"}},{"cell_type":"code","source":["BATCH_SIZE = 128\n","ID_list = [1,2,3,4,5,6]\n","model_name_list = ['mlp', 'SVC']\n","save_number = 0\n","\n","for m_idx in range(1):\n","  model_list = []\n","  for ID in range(1, 7):\n","    model_name = result_path_isolated + '/'+ model_name_list[m_idx] + '_sub00' + str(ID) + '.pck'\n","    model_stroke = pickle.load(open(model_name, \"rb\"))\n","    model_list.append(model_stroke)\n","\n","  score1 = np.zeros((4, 3, 8))\n","  score2 = np.zeros((4, 3, 8))\n","  for fold_idx in range(1, 5):\n","    test_dataset = MyDataset(ID_list, R_time_list=[fold_idx], model_list=model_list, flatten=True, model_type=0)\n","    test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    x = fold_idx+1 if fold_idx+1<=5 else 1\n","    val_dataset = MyDataset(ID_list, R_time_list=[x], model_list=model_list, flatten=True, model_type=0)\n","    val_loader = Data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    R_time_list = []\n","    for i in range(1, 9):\n","      if i!=fold_idx and i!=x:\n","        R_time_list.append(i)\n","    train_dataset = MyDataset(ID_list, R_time_list=R_time_list, model_list=model_list, flatten=True, model_type=0)\n","    train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","      \n","    model_param=0\n","    score1[fold_idx-1] = main_function(total_epoch=200, dataloader_list=[train_loader, val_loader, test_loader], \\\n","                                  model_para=model_param, model_save=False, cross_val=True, weight_decay=1e-4)\n","    print('A fold{} Train Loss{}, Train Acc{}'.format(fold_idx, score1[fold_idx-1, 0, 0], score1[fold_idx-1, 0, 1]))\n","    print('A fold{} Val Loss{}, Val Acc{}'.format(fold_idx, score1[fold_idx-1, 1, 0], score1[fold_idx-1, 1, 1]))\n","    print('A fold{} Test Loss{}, Test Acc{}'.format(fold_idx, score1[fold_idx-1, 2, 0], score1[fold_idx-1, 2, 1]))\n","    torch.cuda.empty_cache()\n","\n","    model_param=1\n","    score2[fold_idx-1] = main_function(total_epoch=200, dataloader_list=[train_loader, val_loader, test_loader], \\\n","                                  model_para=model_param, model_save=False, cross_val=True, weight_decay=1e-4)\n","    print('B fold{} Train Loss{}, Train Acc{}'.format(fold_idx, score2[fold_idx-1, 0, 0], score2[fold_idx-1, 0, 1]))\n","    print('B fold{} Val Loss{}, Val Acc{}'.format(fold_idx, score2[fold_idx-1, 1, 0], score2[fold_idx-1, 1, 1]))\n","    print('B fold{} Test Loss{}, Test Acc{}'.format(fold_idx, score2[fold_idx-1, 2, 0], score2[fold_idx-1, 2, 1]))\n","    torch.cuda.empty_cache()\n","\n","  np.save(result_path + '/d0425_SPLIT_A_score_' + str(save_number), score1)\n","  save_number += 1\n","  np.save(result_path + '/d0425_SPLIT_B_score_' + str(save_number), score2)\n","  save_number += 1\n","\n","  x = np.mean(score1, axis=0)\n","  print('Train', x[0])\n","  print('Val', x[1])\n","  print('Test', x[2])\n","\n","  x = np.mean(score2, axis=0)\n","  print('Train', x[0])\n","  print('Val', x[1])\n","  print('Test', x[2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5eXyYvkcloo3","executionInfo":{"status":"ok","timestamp":1682435628806,"user_tz":240,"elapsed":641021,"user":{"displayName":"戴劲","userId":"11036111496252500300"}},"outputId":"f50ce230-04fb-4d2a-9e6c-ba00662320f2"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [01:02<00:00,  3.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["A fold1 Train Loss0.2605916437777606, Train Acc0.9930555555555556\n","A fold1 Val Loss3.1531545221805573, Val Acc0.28888888888888886\n","A fold1 Test Loss3.3566650450229645, Test Acc0.28555555555555556\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["B fold1 Train Loss0.001067140787861056, Train Acc1.0\n","B fold1 Val Loss0.3925686225993559, Val Acc0.9033333333333333\n","B fold1 Test Loss0.34193918216715247, Test Acc0.9122222222222223\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [01:04<00:00,  3.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["A fold2 Train Loss0.5105223154479807, Train Acc0.9674707602339181\n","A fold2 Val Loss3.3710907995700836, Val Acc0.30444444444444446\n","A fold2 Test Loss3.1108473241329193, Test Acc0.30666666666666664\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [01:17<00:00,  2.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["B fold2 Train Loss0.0006505438994446939, Train Acc1.0\n","B fold2 Val Loss0.3559082861756906, Val Acc0.92\n","B fold2 Test Loss0.2838261618344404, Test Acc0.9211111111111111\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [01:03<00:00,  3.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["A fold3 Train Loss0.2697588245977055, Train Acc0.9908659115820241\n","A fold3 Val Loss3.058923050761223, Val Acc0.303670745272525\n","A fold3 Test Loss3.321137398481369, Test Acc0.30666666666666664\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [01:16<00:00,  2.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["B fold3 Train Loss0.0009762187784707004, Train Acc1.0\n","B fold3 Val Loss0.33548669230003725, Val Acc0.9043381535038932\n","B fold3 Test Loss0.41508640348911285, Test Acc0.91\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [01:04<00:00,  3.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["A fold4 Train Loss0.48054294423623517, Train Acc0.9762687112084703\n","A fold4 Val Loss3.070296809077263, Val Acc0.2661469933184855\n","A fold4 Test Loss2.982828974723816, Test Acc0.3114571746384872\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [01:17<00:00,  2.60it/s]"]},{"output_type":"stream","name":"stdout","text":["B fold4 Train Loss0.0011854521237017418, Train Acc1.0\n","B fold4 Val Loss0.3057566001371015, Val Acc0.9142538975501113\n","B fold4 Test Loss0.3104901406913996, Test Acc0.9199110122358176\n","Train [0.38035393 0.98191523 0.98191523 0.98197705 0.98191523 0.98286258\n"," 0.98191523 0.98206509]\n","Val [3.1633663  0.29078777 0.29078777 0.28503662 0.29078777 0.30399916\n"," 0.29078777 0.29083333]\n","Test [3.19286969 0.30258652 0.30258652 0.29483052 0.30258652 0.31439798\n"," 0.30258652 0.30255556]\n","Train [9.69838897e-04 1.00000000e+00 1.00000000e+00 1.00000000e+00\n"," 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00]\n","Val [0.34743005 0.91048135 0.91048135 0.90989254 0.91048135 0.92014286\n"," 0.91048135 0.9105    ]\n","Test [0.33783547 0.91581109 0.91581109 0.91579217 0.91581109 0.92727513\n"," 0.91581109 0.91583333]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyPHLspyS3pcW6vOe+8NyXfc"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
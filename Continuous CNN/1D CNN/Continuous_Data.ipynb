{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGJOJ1vZS_HF"
      },
      "source": [
        "# import "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZbMK-khTIsd",
        "outputId": "fab0605f-19d4-487d-fc7b-26abb9fd1ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78JZVMvWT1Nl"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/MLMA/project/Code Continuous Data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaSlW2eobWLs"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from stroke_probability_map import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cWpoQ2uS_HH",
        "outputId": "262aba92-4acb-4c43-9159-8b85eb31a644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from stroke_probability_map_1 import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stgVSD3US_HI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from scipy import signal\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data as Data\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INdrL3EhQ0wf"
      },
      "source": [
        "# File and Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR4iBbuGS_HI"
      },
      "source": [
        "File name:   \n",
        "EOG_00x_xx_xxx.csv:   \n",
        " 00x: participant's ID.    \n",
        " xx:  index of the recording time.    \n",
        " xxx: index of the corresponding eye movements.    \n",
        "      - e.g., xxx is the index of a training word.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EIba5Pg2Zcq"
      },
      "outputs": [],
      "source": [
        "result_path = '/content/drive/MyDrive/Colab Notebooks/MLMA/project/Code Continuous Data/Result'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qieqdMYZX17L"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/MyDrive/Colab Notebooks/MLMA/project/Data/continuous'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWC2DhqhrkeI"
      },
      "outputs": [],
      "source": [
        "result_path_isolated = '/content/drive/MyDrive/Colab Notebooks/MLMA/project/Code Isolated Data/Result'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtG9GXcaW3bo"
      },
      "source": [
        "# Data explore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK0VCNexdaq0",
        "outputId": "57a002ed-f1d9-42d0-e1b0-69d267b0e1e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "756"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lrd8mMoW57_"
      },
      "outputs": [],
      "source": [
        "frequency = np.zeros((6, 150))\n",
        "length = []\n",
        "for ID in range(1, 7):\n",
        "  c = 0\n",
        "  p = data_path + '/00' + str(ID) +'/training/'\n",
        "  file_list = os.listdir(p)\n",
        "  length.append([])\n",
        "  for i in range(len(file_list)):\n",
        "    if '.csv' in file_list[i]:\n",
        "      a = pd.read_csv(p + file_list[i])\n",
        "      length[-1].append(a.shape[0])\n",
        "      frequency[ID-1, int(file_list[i][-7:-4])-1] += 1\n",
        "      c+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtNFZKV5eDMZ"
      },
      "outputs": [],
      "source": [
        "t,f = np.unique(frequency, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDsHGuayLFqU",
        "outputId": "b029e4d6-7cd2-42e7-9698-ccb3a86c38bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3. 4. 5. 6. 7. 8.]\n",
            "[  1   1 868  23   5   2]\n"
          ]
        }
      ],
      "source": [
        "print(t)\n",
        "print(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65jItQq4eNk4",
        "outputId": "3b1e923b-59aa-4c43-d3cb-93b65f225366"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "224 599 422.72266666666667\n",
            "136 486 279.0733333333333\n",
            "149 536 317.271164021164\n",
            "211 836 439.542216358839\n",
            "274 1136 607.5059132720105\n",
            "161 761 403.9658344283837\n"
          ]
        }
      ],
      "source": [
        "for i in range(6):\n",
        "  print(np.min(length[i]), np.max(length[i]), np.mean(length[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXaoEpn8PfPG"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WssXMkpEPi7a"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, ID_list, R_time_list, model_list, flatten, model_type, root_dir='/content/drive/MyDrive/Colab Notebooks/MLMA/project/Data/continuous/00'):\n",
        "    self.data = []\n",
        "    self.stroke_probability_map = []\n",
        "    self.label = np.zeros((0,))\n",
        "    for id_idx in range(len(ID_list)):\n",
        "      file_list = os.listdir(root_dir + str(ID_list[id_idx]) +'/training/')\n",
        "      for f_idx in range(len(file_list)):\n",
        "        if int(file_list[f_idx][8:10]) in R_time_list:\n",
        "          data = pd.read_csv(root_dir + str(ID_list[id_idx]) +'/training/' + file_list[f_idx],  header=None).to_numpy()\n",
        "          #print(data.shape)\n",
        "          data = signal.resample(data, 1024, axis=0)\n",
        "          #print(data.shape)\n",
        "          s = stroke_probability_map_1(data, model_list[id_idx], flatten, model_type)\n",
        "          s = torch.from_numpy(s)\n",
        "          self.stroke_probability_map.append(s)\n",
        "          self.label = np.append(self.label, np.ones((1,))*(int(file_list[f_idx][-7:-4])-1), axis=0)\n",
        "          data = (data - data.mean(axis=0, keepdims=True))/np.std(data,axis=0, keepdims=True)\n",
        "          data = torch.from_numpy(data)\n",
        "          self.data.append(data)\n",
        "\n",
        "    self.label = torch.from_numpy(self.label)\n",
        "  def __getitem__(self, index) :\n",
        "    return self.data[index].float(), self.stroke_probability_map[index].float(), self.label[index].type(torch.LongTensor)\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78A6FXCrT_v0"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1biLuC7aUwi"
      },
      "source": [
        "https://www.cnblogs.com/marsggbo/p/8572846.html\n",
        "\n",
        "SPP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR-zhR1zl0pT"
      },
      "outputs": [],
      "source": [
        "# 一般\n",
        "class My_CNN_1_simplify2(nn.Module):\n",
        "  def __init__(self, N=16):\n",
        "    super(My_CNN_1_simplify2, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels=2, out_channels=8, kernel_size=9, padding=5)\n",
        "    self.mp1 = nn.MaxPool1d(kernel_size=4)\n",
        "    self.relu1 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv1d(in_channels=8, out_channels=8, kernel_size=5, padding=3)\n",
        "    self.mp2 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu2 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv3 = nn.Conv1d(in_channels=8, out_channels=8, kernel_size=3, padding=2)\n",
        "    self.mp3 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu3 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv4 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=2)\n",
        "    self.amp =  nn.AdaptiveMaxPool1d(N)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "\n",
        "    self.fc1 = nn.Linear(int(3*12+16), 25)\n",
        "    self.relu5 = nn.LeakyReLU()\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    self.fc2 = nn.Linear(25*16, 150)\n",
        "\n",
        "\n",
        "  def forward(self, x, probability):\n",
        "    x = torch.transpose(x, -1, -2)\n",
        "\n",
        "    x = self.relu1(self.mp1(self.conv1(x)))\n",
        "    x = self.relu2(self.mp2(self.conv2(x)))\n",
        "    x = self.relu3(self.mp3(self.conv3(x)))\n",
        "    x = self.conv4(x)\n",
        "    x = self.sigmoid(self.amp(x))\n",
        "    x = torch.transpose(x, -1, -2)\n",
        "    x = torch.cat((x, probability), dim=-1)\n",
        "\n",
        "    x = self.relu5(self.fc1(x))\n",
        "    x = self.flatten(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdSEy97Mk87F"
      },
      "outputs": [],
      "source": [
        "# Overfitting 好一些\n",
        "class My_CNN_1_simplify(nn.Module):\n",
        "  def __init__(self, N=16):\n",
        "    super(My_CNN_1_simplify, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels=2, out_channels=8, kernel_size=9, padding=5)\n",
        "    self.mp1 = nn.MaxPool1d(kernel_size=4)\n",
        "    self.relu1 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=5, padding=3)\n",
        "    self.mp2 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu2 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv3 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=2)\n",
        "    self.mp3 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu3 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv4 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=2)\n",
        "    self.amp =  nn.AdaptiveMaxPool1d(N)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "\n",
        "    self.fc1 = nn.Linear(int(3*12+32), 25)\n",
        "    self.relu5 = nn.LeakyReLU()\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    self.fc2 = nn.Linear(25*16, 150)\n",
        "\n",
        "\n",
        "  def forward(self, x, probability):\n",
        "    x = torch.transpose(x, -1, -2)\n",
        "\n",
        "    x = self.relu1(self.mp1(self.conv1(x)))\n",
        "    x = self.relu2(self.mp2(self.conv2(x)))\n",
        "    x = self.relu3(self.mp3(self.conv3(x)))\n",
        "    x = self.conv4(x)\n",
        "    x = self.sigmoid(self.amp(x))\n",
        "    x = torch.transpose(x, -1, -2)\n",
        "    x = torch.cat((x, probability), dim=-1)\n",
        "\n",
        "    x = self.relu5(self.fc1(x))\n",
        "    x = self.flatten(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 比3复杂\n",
        "class My_CNN_3(nn.Module):\n",
        "  def __init__(self, N=16):\n",
        "    super(My_CNN_3, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels=2, out_channels=32, kernel_size=9, padding=5)\n",
        "    nn.init.xavier_uniform_(self.conv1.weight)\n",
        "    nn.init.constant_(self.conv1.bias, 0.0)\n",
        "    self.mp1 = nn.MaxPool1d(kernel_size=4)\n",
        "    self.relu1 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=3)\n",
        "    nn.init.xavier_uniform_(self.conv2.weight)\n",
        "    nn.init.constant_(self.conv2.bias, 0.0)\n",
        "    self.mp2 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu2 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=2)\n",
        "    nn.init.xavier_uniform_(self.conv3.weight)\n",
        "    nn.init.constant_(self.conv3.bias, 0.0)\n",
        "    self.mp3 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu3 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv4 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=2)\n",
        "    nn.init.xavier_uniform_(self.conv4.weight)\n",
        "    nn.init.constant_(self.conv4.bias, 0.0)\n",
        "    self.amp =  nn.AdaptiveMaxPool1d(N)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "\n",
        "    self.fc1 = nn.Linear(int(3*12+256), 25)\n",
        "    nn.init.xavier_uniform_(self.fc1.weight)\n",
        "    nn.init.constant_(self.fc1.bias, 0.0)\n",
        "    self.relu5 = nn.LeakyReLU()\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    self.fc2 = nn.Linear(25*16, 150)\n",
        "    nn.init.xavier_uniform_(self.fc2.weight)\n",
        "    nn.init.constant_(self.fc2.bias, 0.0)\n",
        "\n",
        "  def forward(self, x, probability):\n",
        "    x = torch.transpose(x, -1, -2)\n",
        "\n",
        "    x = self.relu1(self.mp1(self.conv1(x)))\n",
        "    x = self.relu2(self.mp2(self.conv2(x)))\n",
        "    x = self.relu3(self.mp3(self.conv3(x)))\n",
        "    x = self.conv4(x)\n",
        "    x = self.sigmoid(self.amp(x))\n",
        "    x = torch.transpose(x, -1, -2)\n",
        "    x = torch.cat((x, probability), dim=-1)\n",
        "\n",
        "    x = self.relu5(self.fc1(x))\n",
        "    x = self.flatten(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "Z4zIpppFCrpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2+lstm\n",
        "class My_CNN_2_lstm(nn.Module):\n",
        "  def __init__(self, N=16):\n",
        "    super(My_CNN_2_lstm, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels=2, out_channels=16, kernel_size=9, padding=5)\n",
        "    nn.init.xavier_uniform_(self.conv1.weight)\n",
        "    nn.init.constant_(self.conv1.bias, 0.0)\n",
        "    self.mp1 = nn.MaxPool1d(kernel_size=4)\n",
        "    self.relu1 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=3)\n",
        "    nn.init.xavier_uniform_(self.conv2.weight)\n",
        "    nn.init.constant_(self.conv2.bias, 0.0)\n",
        "    self.mp2 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu2 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=2)\n",
        "    nn.init.xavier_uniform_(self.conv3.weight)\n",
        "    nn.init.constant_(self.conv3.bias, 0.0)\n",
        "    self.mp3 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu3 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=2)\n",
        "    nn.init.xavier_uniform_(self.conv4.weight)\n",
        "    nn.init.constant_(self.conv4.bias, 0.0)\n",
        "    self.amp =  nn.AdaptiveMaxPool1d(N)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size=int(3*12+128), hidden_size=25, num_layers=1, batch_first=True, bidirectional=False, dropout=0.5)\n",
        "    for name, param in self.lstm.named_parameters():\n",
        "      if name.startswith(\"weight\"):\n",
        "        nn.init.xavier_normal_(param)\n",
        "      else:\n",
        "        nn.init.zeros_(param)\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    self.fc2 = nn.Linear(25*16, 150)\n",
        "    nn.init.xavier_uniform_(self.fc2.weight)\n",
        "    nn.init.constant_(self.fc2.bias, 0.0)\n",
        "\n",
        "  def forward(self, x, probability):\n",
        "    x = torch.transpose(x, -1, -2)\n",
        "\n",
        "    x = self.relu1(self.mp1(self.conv1(x)))\n",
        "    x = self.relu2(self.mp2(self.conv2(x)))\n",
        "    x = self.relu3(self.mp3(self.conv3(x)))\n",
        "    x = self.conv4(x)\n",
        "    x = self.sigmoid(self.amp(x))\n",
        "    x = torch.transpose(x, -1, -2)\n",
        "    x = torch.cat((x, probability), dim=-1)\n",
        "\n",
        "    x, (_, _) = self.lstm(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "6_KYC7lfgt1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 比1复杂\n",
        "class My_CNN_2(nn.Module):\n",
        "  def __init__(self, N=16):\n",
        "    super(My_CNN_2, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels=2, out_channels=16, kernel_size=9, padding=5)\n",
        "    nn.init.xavier_uniform_(self.conv1.weight)\n",
        "    nn.init.constant_(self.conv1.bias, 0.0)\n",
        "    self.mp1 = nn.MaxPool1d(kernel_size=4)\n",
        "    self.relu1 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=3)\n",
        "    nn.init.xavier_uniform_(self.conv2.weight)\n",
        "    nn.init.constant_(self.conv2.bias, 0.0)\n",
        "    self.mp2 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu2 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=2)\n",
        "    nn.init.xavier_uniform_(self.conv3.weight)\n",
        "    nn.init.constant_(self.conv3.bias, 0.0)\n",
        "    self.mp3 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu3 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=2)\n",
        "    nn.init.xavier_uniform_(self.conv4.weight)\n",
        "    nn.init.constant_(self.conv4.bias, 0.0)\n",
        "    self.amp =  nn.AdaptiveMaxPool1d(N)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "\n",
        "    self.fc1 = nn.Linear(int(3*12+128), 25)\n",
        "    nn.init.xavier_uniform_(self.fc1.weight)\n",
        "    nn.init.constant_(self.fc1.bias, 0.0)\n",
        "    self.relu5 = nn.LeakyReLU()\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    self.fc2 = nn.Linear(25*16, 150)\n",
        "    nn.init.xavier_uniform_(self.fc2.weight)\n",
        "    nn.init.constant_(self.fc2.bias, 0.0)\n",
        "\n",
        "  def forward(self, x, probability):\n",
        "    x = torch.transpose(x, -1, -2)\n",
        "\n",
        "    x = self.relu1(self.mp1(self.conv1(x)))\n",
        "    x = self.relu2(self.mp2(self.conv2(x)))\n",
        "    x = self.relu3(self.mp3(self.conv3(x)))\n",
        "    x = self.conv4(x)\n",
        "    x = self.sigmoid(self.amp(x))\n",
        "    x = torch.transpose(x, -1, -2)\n",
        "    x = torch.cat((x, probability), dim=-1)\n",
        "\n",
        "    x = self.relu5(self.fc1(x))\n",
        "    x = self.flatten(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "m08jdMeST4FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QBJ-deVUBSU"
      },
      "outputs": [],
      "source": [
        "# Train 收敛效果好，但是overfitting严重\n",
        "class My_CNN_1(nn.Module):\n",
        "  def __init__(self, N=16):\n",
        "    super(My_CNN_1, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels=2, out_channels=16, kernel_size=9, padding=5)\n",
        "    self.mp1 = nn.MaxPool1d(kernel_size=4)\n",
        "    self.relu1 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=3)\n",
        "    self.mp2 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu2 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv3 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=2)\n",
        "    self.mp3 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu3 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv4 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=2)\n",
        "    self.amp =  nn.AdaptiveMaxPool1d(N)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "\n",
        "    self.fc1 = nn.Linear(int(3*12+32), 25)\n",
        "    self.relu5 = nn.LeakyReLU()\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    self.fc2 = nn.Linear(25*16, 150)\n",
        "\n",
        "\n",
        "  def forward(self, x, probability):\n",
        "    x = torch.transpose(x, -1, -2)\n",
        "\n",
        "    x = self.relu1(self.mp1(self.conv1(x)))\n",
        "    x = self.relu2(self.mp2(self.conv2(x)))\n",
        "    x = self.relu3(self.mp3(self.conv3(x)))\n",
        "    x = self.conv4(x)\n",
        "    x = self.sigmoid(self.amp(x))\n",
        "    x = torch.transpose(x, -1, -2)\n",
        "    x = torch.cat((x, probability), dim=-1)\n",
        "\n",
        "    x = self.relu5(self.fc1(x))\n",
        "    x = self.flatten(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KztcCgr3jFtP"
      },
      "source": [
        "### x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIDNIsrfK0n4"
      },
      "outputs": [],
      "source": [
        "class My_CNN_2(nn.Module):\n",
        "  def __init__(self, N=10):\n",
        "    super(My_CNN_2, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels=2, out_channels=16, kernel_size=3, padding=2)\n",
        "    #self.mp1 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu1 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=2)\n",
        "    #self.mp2 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu2 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv3 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=2)\n",
        "    #self.mp3 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu3 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv4 = nn.Conv1d(in_channels=32, out_channels=36, kernel_size=3, padding=2)\n",
        "    self.amp =  nn.AdaptiveMaxPool1d(N)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    self.fc1 = nn.Linear(int(45*12+N*36), 512)\n",
        "    self.relu5 = nn.LeakyReLU()\n",
        "\n",
        "    self.fc2 = nn.Linear(512, 150)\n",
        "\n",
        "\n",
        "  def forward(self, x, probability):\n",
        "    probability = self.flatten(probability)\n",
        "\n",
        "    x = torch.transpose(x, -1, -2)\n",
        "\n",
        "    x = self.relu1(self.conv1(x))\n",
        "    x = self.relu2(self.conv2(x))\n",
        "    x = self.relu3(self.conv3(x))\n",
        "    x = self.conv4(x)\n",
        "    x = self.sigmoid(self.amp(x))\n",
        "    x = self.flatten(x)\n",
        "    x = torch.cat((x, probability), dim=-1)\n",
        "\n",
        "    x = self.relu5(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL6H6S_-05CZ"
      },
      "outputs": [],
      "source": [
        "class My_CNN_original(nn.Module):\n",
        "  def __init__(self, N=10):\n",
        "    super(My_CNN_original, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels=2, out_channels=16, kernel_size=3, padding=2)\n",
        "    self.mp1 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu1 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=2)\n",
        "    self.mp2 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu2 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv3 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=2)\n",
        "    self.mp3 = nn.MaxPool1d(kernel_size=2)\n",
        "    self.relu3 = nn.LeakyReLU()\n",
        "\n",
        "    self.conv4 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=2)\n",
        "    self.amp =  nn.AdaptiveMaxPool1d(N)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    self.fc1 = nn.Linear(int(45*12+N*32), 512)\n",
        "    self.relu5 = nn.LeakyReLU()\n",
        "\n",
        "    self.fc2 = nn.Linear(512, 150)\n",
        "\n",
        "\n",
        "  def forward(self, x, probability):\n",
        "    probability = self.flatten(probability)\n",
        "\n",
        "    x = torch.transpose(x, -1, -2)\n",
        "\n",
        "    x = self.relu1(self.mp1(self.conv1(x)))\n",
        "    x = self.relu2(self.mp2(self.conv2(x)))\n",
        "    x = self.relu3(self.mp3(self.conv3(x)))\n",
        "    x = self.conv4(x)\n",
        "    x = self.sigmoid(self.amp(x))\n",
        "    x = self.flatten(x)\n",
        "    x = torch.cat((x, probability), dim=-1)\n",
        "\n",
        "    x = self.relu5(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVydt1AQo0Fs"
      },
      "source": [
        "# Some Funtions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w7ojQc8o-mm"
      },
      "outputs": [],
      "source": [
        "def Train_model(model,epochs,optmizer,criterion, train_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    x = 0\n",
        "    for batch_idx, (data, probability, label) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        probability = probability.to(device)\n",
        "        optmizer.zero_grad()  \n",
        "        output = model(data, probability)\n",
        "        # print(type(output), type(label))\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optmizer.step() \n",
        "        train_loss.append(loss.item()) \n",
        "        #if(batch_idx+1)%3 == 0:\n",
        "            #print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.\\\n",
        "                #format(epochs, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(train_loss), model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozKFJi50pKec"
      },
      "outputs": [],
      "source": [
        "def test(model,epochs,criterion, dataloader, T_T='Test'):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    c = 0\n",
        "    y_pred = np.zeros((0,))\n",
        "    y_true = np.zeros((0, ))\n",
        "    with torch.no_grad():\n",
        "        for data, probability, label in dataloader:\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            probability = probability.to(device)\n",
        "            output = model(data, probability)\n",
        "            test_loss += criterion(output, label).item()\n",
        "            y_pred = np.append(y_pred, output.cpu().argmax(dim=-1), axis=0)\n",
        "            y_true = np.append(y_true, label.cpu(), axis=0)\n",
        "            c += 1\n",
        "    test_loss /= c\n",
        "    #if epochs%50 == 0:\n",
        "        #print('EPOCH:{}    {} set: Average loss: {:.4f}'.format(epochs, T_T, test_loss))\n",
        "  \n",
        "    return test_loss, accuracy_score(y_true, y_pred), f1_score(y_true, y_pred, average='micro', zero_division=0), \\\n",
        "    f1_score(y_true, y_pred, average='macro', zero_division=0), precision_score(y_true, y_pred, average='micro', zero_division=0), \\\n",
        "    precision_score(y_true, y_pred, average='macro', zero_division=0), recall_score(y_true, y_pred, average='micro', zero_division=0), \\\n",
        "    recall_score(y_true, y_pred, average='macro', zero_division=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW7yqn0wpRYf"
      },
      "outputs": [],
      "source": [
        "def main_function(total_epoch=100, dataloader_list=None, model_para=None, model_save=False, model_name=None, cross_val=True, weight_decay=1e-3):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    model = My_CNN_2_lstm().to(device)\n",
        "        \n",
        "    optmizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=weight_decay)  \n",
        "    #optmizer = optim.SGD(model.parameters(), lr=1e-3, weight_decay=weight_decay) \n",
        "    scheduler = lr_scheduler.StepLR(optmizer, step_size=50, gamma=0.5, verbose=False) \n",
        "    criterion = nn.CrossEntropyLoss().to(device)  \n",
        "    \n",
        "    train_loss = np.zeros((total_epoch))\n",
        "    score = np.zeros((len(dataloader_list), 8, total_epoch)) # loss, accuracy, f1 score*2, precision*2, recall*2\n",
        "    \n",
        "    for i in tqdm(range(total_epoch)):\n",
        "        train_loss[i], model = Train_model(model, i, optmizer, criterion, dataloader_list[0])\n",
        "        scheduler.step()\n",
        "        for j in range(len(dataloader_list)):\n",
        "            if j == 0:\n",
        "                T_T = 'Train'\n",
        "            elif j == 1:\n",
        "                T_T = 'Validation'\n",
        "            else:\n",
        "                T_T = 'Test'\n",
        "            score[j, :, i] = test(model, i, criterion, dataloader_list[j], T_T=T_T)\n",
        "        #if np.argmin(score[1, 0, :(i+1)]) == i:\n",
        "            #if model_save:\n",
        "                #torch.save(model.state_dict(), model_name)     \n",
        "    #if model_save: \n",
        "        #np.savez(model_name[:-4], train_loss=train_loss, score=score)\n",
        "    if model_save:\n",
        "      torch.save(model.state_dict(), model_name)\n",
        "    if cross_val:\n",
        "        epoch_best = np.argmin(score[1, 0, :])\n",
        "        return score[:, :, epoch_best]\n",
        "    else:\n",
        "        return train_loss, score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHhV-NanphiN"
      },
      "source": [
        " # User Dependent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGEm4I9mpmtL"
      },
      "outputs": [],
      "source": [
        "score = np.zeros((6, 5, 3, 8))\n",
        "BATCH_SIZE = 128\n",
        "for ID in range(1, 7):\n",
        "  model_name = result_path_isolated + '/mlp_sub00' + str(ID) + '.pck'\n",
        "  model_stroke = pickle.load(open(model_name, \"rb\"))\n",
        "\n",
        "  for fold_idx in range(1, 6):\n",
        "    test_dataset = MyDataset(ID_list=[ID], R_time_list=[fold_idx], model_list=[model_stroke], flatten=True, model_type=0)\n",
        "    test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "    x = fold_idx+1 if fold_idx+1<=5 else 1\n",
        "    val_dataset = MyDataset(ID_list=[ID], R_time_list=[x], model_list=[model_stroke], flatten=True, model_type=0)\n",
        "    val_loader = Data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "    R_time_list = []\n",
        "    for i in range(1, 9):\n",
        "      if i!=fold_idx and i!=x:\n",
        "        R_time_list.append(i)\n",
        "    train_dataset = MyDataset(ID_list=[ID], R_time_list=R_time_list, model_list=[model_stroke], flatten=True, model_type=0)\n",
        "    train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "    model_param=64\n",
        "    score[ID-1, fold_idx-1] = main_function(total_epoch=300, dataloader_list=[train_loader, val_loader, test_loader], \\\n",
        "                            model_para=model_param, model_save=False, cross_val=True)\n",
        "    print('ID{} fold{} Train Loss{}, Train Acc{}'.format(ID, fold_idx, score[ID-1, fold_idx-1, 0, 0], score[ID-1, fold_idx-1, 0, 1]))\n",
        "    print('ID{} fold{} Val Loss{}, Val Acc{}'.format(ID, fold_idx, score[ID-1, fold_idx-1, 1, 0], score[ID-1, fold_idx-1, 1, 1]))\n",
        "    print('ID{} fold{} Test Loss{}, Test Acc{}'.format(ID, fold_idx, score[ID-1, fold_idx-1, 2, 0], score[ID-1, fold_idx-1, 2, 1]))\n",
        "    torch.cuda.empty_cache()\n",
        "    break\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jTTmsVapkos"
      },
      "source": [
        "# User Dependent 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***d0419***"
      ],
      "metadata": {
        "id": "z6-KZHRWldBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leave one trial out"
      ],
      "metadata": {
        "id": "fHWYiUTVdm3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhG77uuaEPRF"
      },
      "outputs": [],
      "source": [
        "score = np.zeros((4, 3, 8))\n",
        "BATCH_SIZE = 128\n",
        "ID_list = [1,2,3,4,5,6]\n",
        "model_list = []\n",
        "for ID in range(1, 7):\n",
        "  model_name = result_path_isolated + '/SVC_sub00' + str(ID) + '.pck'\n",
        "  model_stroke = pickle.load(open(model_name, \"rb\"))\n",
        "  model_list.append(model_stroke)\n",
        "\n",
        "\n",
        "for fold_idx in range(1, 5):\n",
        "  test_dataset = MyDataset(ID_list, R_time_list=[fold_idx], model_list=model_list, flatten=True, model_type=0)\n",
        "  test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "  x = fold_idx+1 if fold_idx+1<=5 else 1\n",
        "  val_dataset = MyDataset(ID_list, R_time_list=[x], model_list=model_list, flatten=True, model_type=0)\n",
        "  val_loader = Data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "  R_time_list = []\n",
        "  for i in range(1, 9):\n",
        "    if i!=fold_idx and i!=x:\n",
        "      R_time_list.append(i)\n",
        "  train_dataset = MyDataset(ID_list, R_time_list=R_time_list, model_list=model_list, flatten=True, model_type=0)\n",
        "  train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "  model_param=64\n",
        "  score[fold_idx-1] = main_function(total_epoch=500, dataloader_list=[train_loader, val_loader, test_loader], \\\n",
        "                              model_para=model_param, model_save=False, cross_val=True)\n",
        "  print('fold{} Train Loss{}, Train Acc{}'.format(fold_idx, score[fold_idx-1, 0, 0], score[fold_idx-1, 0, 1]))\n",
        "  print('fold{} Val Loss{}, Val Acc{}'.format(fold_idx, score[fold_idx-1, 1, 0], score[fold_idx-1, 1, 1]))\n",
        "  print('fold{} Test Loss{}, Test Acc{}'.format(fold_idx, score[fold_idx-1, 2, 0], score[fold_idx-1, 2, 1]))\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "np.save(result_path + '/d0419_score_7', score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**weight_decay = 1e-3**\n",
        "\n"
      ],
      "metadata": {
        "id": "S9HIfyvSp6wZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP"
      ],
      "metadata": {
        "id": "jQ8zn1t5157_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# My_CNN_1\n",
        "x = np.mean(score, axis=0)\n",
        "print('Train', x[0])\n",
        "print('Val', x[1])\n",
        "print('Test', x[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ChvO6ecR_dq",
        "outputId": "15d3a6d2-5658-4591-e103-56ad3afa53f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train [0.02044575 1.         1.         1.         1.         1.\n",
            " 1.         1.        ]\n",
            "Val [0.55072433 0.84569883 0.84569883 0.84499433 0.84569883 0.86426112\n",
            " 0.84569883 0.84566667]\n",
            "Test [0.54708268 0.84857156 0.84857156 0.84705738 0.84857156 0.86606223\n",
            " 0.84857156 0.84855556]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# My_CNN_1_simplify\n",
        "x = np.mean(score, axis=0)\n",
        "print('Train', x[0])\n",
        "print('Val', x[1])\n",
        "print('Test', x[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sOJbBloo0ny",
        "outputId": "4fb41f3d-3a66-4196-e95c-0a3d3510826b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train [0.02857823 1.         1.         1.         1.         1.\n",
            " 1.         1.        ]\n",
            "Val [0.72329674 0.80039631 0.80039631 0.79769884 0.80039631 0.81597817\n",
            " 0.80039631 0.80044444]\n",
            "Test [0.71039559 0.80328606 0.80328606 0.80174082 0.80328606 0.82290873\n",
            " 0.80328606 0.80327778]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# My_CNN_2\n",
        "x = np.mean(score, axis=0)\n",
        "print('Train', x[0])\n",
        "print('Val', x[1])\n",
        "print('Test', x[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0CUGy9gr0zy",
        "outputId": "6f8c9761-7e3c-482b-9b27-4847049df0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train [0.003943 1.       1.       1.       1.       1.       1.       1.      ]\n",
            "Val [0.26003276 0.92354711 0.92354711 0.92340685 0.92354711 0.93439322\n",
            " 0.92354711 0.9235    ]\n",
            "Test [0.25467589 0.92747961 0.92747961 0.92707469 0.92747961 0.93722421\n",
            " 0.92747961 0.92744444]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVC"
      ],
      "metadata": {
        "id": "kiyHFPLF0G2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# My_CNN_2\n",
        "x = np.mean(score, axis=0)\n",
        "print('Train', x[0])\n",
        "print('Val', x[1])\n",
        "print('Test', x[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVhM3AKT0F7B",
        "outputId": "dd02c9d8-4308-4a20-a73e-5e280ea1a950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train [1.25780054 0.75164294 0.75164294 0.75002176 0.75164294 0.75001095\n",
            " 0.75164294 0.75166667]\n",
            "Val [1.53292642 0.67551848 0.67551848 0.67342644 0.67551848 0.6840946\n",
            " 0.67551848 0.67583333]\n",
            "Test [1.54590309 0.67333519 0.67333519 0.67133243 0.67333519 0.6803438\n",
            " 0.67333519 0.67333333]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**weight_decay 2e-3, 5e-3**"
      ],
      "metadata": {
        "id": "C8WctUhF1xRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "ID_list = [1,2,3,4,5,6]\n",
        "weight_decay_list = [2e-3, 5e-3]\n",
        "\n",
        "model_name_list = ['mlp', 'SVC']\n",
        "save_number = 8\n",
        "for m_idx in range(2):\n",
        "  model_list = []\n",
        "  for ID in range(1, 7):\n",
        "    model_name = result_path_isolated + '/'+ model_name_list[m_idx] + '_sub00' + str(ID) + '.pck'\n",
        "    model_stroke = pickle.load(open(model_name, \"rb\"))\n",
        "    model_list.append(model_stroke)\n",
        "\n",
        "  for w_idx in range(2):\n",
        "    score = np.zeros((4, 3, 8))\n",
        "    for fold_idx in range(1, 5):\n",
        "      test_dataset = MyDataset(ID_list, R_time_list=[fold_idx], model_list=model_list, flatten=True, model_type=0)\n",
        "      test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "      x = fold_idx+1 if fold_idx+1<=5 else 1\n",
        "      val_dataset = MyDataset(ID_list, R_time_list=[x], model_list=model_list, flatten=True, model_type=0)\n",
        "      val_loader = Data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "      R_time_list = []\n",
        "      for i in range(1, 9):\n",
        "        if i!=fold_idx and i!=x:\n",
        "          R_time_list.append(i)\n",
        "      train_dataset = MyDataset(ID_list, R_time_list=R_time_list, model_list=model_list, flatten=True, model_type=0)\n",
        "      train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "      \n",
        "      model_param=64\n",
        "      score[fold_idx-1] = main_function(total_epoch=300, dataloader_list=[train_loader, val_loader, test_loader], \\\n",
        "                                  model_para=model_param, model_save=False, cross_val=True, \\\n",
        "                                  weight_decay=weight_decay_list[w_idx])\n",
        "      print('fold{} Train Loss{}, Train Acc{}'.format(fold_idx, score[fold_idx-1, 0, 0], score[fold_idx-1, 0, 1]))\n",
        "      print('fold{} Val Loss{}, Val Acc{}'.format(fold_idx, score[fold_idx-1, 1, 0], score[fold_idx-1, 1, 1]))\n",
        "      print('fold{} Test Loss{}, Test Acc{}'.format(fold_idx, score[fold_idx-1, 2, 0], score[fold_idx-1, 2, 1]))\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    np.save(result_path + '/d0419_score_' + str(save_number), score)\n",
        "    save_number += 1\n",
        "\n",
        "    x = np.mean(score, axis=0)\n",
        "    print('Train', x[0])\n",
        "    print('Val', x[1])\n",
        "    print('Test', x[2])"
      ],
      "metadata": {
        "id": "Bzgk6-yq3qlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = ['MLP 2e-3', 'MLP 5e-3', 'SVC 2e-3', 'SVC 5e-3']\n",
        "for i in range(8, 12):\n",
        "  score = np.load(result_path + '/d0419_score_' + str(i) + '.npy')\n",
        "  x = np.mean(score, axis=0)\n",
        "  print(l[i-8])\n",
        "  print('Train', x[0])\n",
        "  print('Val', x[1])\n",
        "  print('Test', x[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgiWIqqMCBFw",
        "outputId": "1fca3d94-fbe0-4f02-def0-4ade42f27586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP 2e-3\n",
            "Train [0.01091593 1.         1.         1.         1.         1.\n",
            " 1.         1.        ]\n",
            "Val [0.28157514 0.92577087 0.92577087 0.9255994  0.92577087 0.93565278\n",
            " 0.92577087 0.92583333]\n",
            "Test [0.29266739 0.91914504 0.91914504 0.91789624 0.91914504 0.92898214\n",
            " 0.91914504 0.91916667]\n",
            "MLP 5e-3\n",
            "Train [0.04894792 0.99972598 0.99972598 0.99973826 0.99972598 0.99974123\n",
            " 0.99972598 0.99974868]\n",
            "Val [0.40448756 0.89046094 0.89046094 0.88947508 0.89046094 0.90389881\n",
            " 0.89046094 0.89044444]\n",
            "Test [0.38996305 0.90164226 0.90164226 0.90106373 0.90164226 0.9141045\n",
            " 0.90164226 0.90161111]\n",
            "SVC 2e-3\n",
            "Train [3.76142782 0.25511632 0.25511632 0.25017655 0.25511632 0.25009078\n",
            " 0.25511632 0.25505361]\n",
            "Val [3.84923661 0.22577054 0.22577054 0.22120742 0.22577054 0.22540468\n",
            " 0.22577054 0.22577778]\n",
            "Test [3.84890309 0.22997126 0.22997126 0.22345567 0.22997126 0.22706479\n",
            " 0.22997126 0.23      ]\n",
            "SVC 5e-3\n",
            "Train [5.01068534e+00 6.39413007e-03 6.39413007e-03 5.26243535e-04\n",
            " 6.39413007e-03 1.06877160e-03 6.39413007e-03 6.29629630e-03]\n",
            "Val [5.01048835e+00 7.22902540e-03 7.22902540e-03 6.52838201e-04\n",
            " 7.22902540e-03 4.39174530e-04 7.22902540e-03 7.22222222e-03]\n",
            "Test [5.01095903e+00 7.50308985e-03 7.50308985e-03 9.34395396e-04\n",
            " 7.50308985e-03 1.92269773e-03 7.50308985e-03 7.50000000e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**My_CNN_3**\n",
        "\n",
        "MLP/SVC"
      ],
      "metadata": {
        "id": "OSLBakmxC8cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "ID_list = [1,2,3,4,5,6]\n",
        "model_name_list = ['mlp', 'SVC']\n",
        "save_number = 12\n",
        "\n",
        "for m_idx in range(2):\n",
        "  model_list = []\n",
        "  for ID in range(1, 7):\n",
        "    model_name = result_path_isolated + '/'+ model_name_list[m_idx] + '_sub00' + str(ID) + '.pck'\n",
        "    model_stroke = pickle.load(open(model_name, \"rb\"))\n",
        "    model_list.append(model_stroke)\n",
        "\n",
        "  score = np.zeros((4, 3, 8))\n",
        "  for fold_idx in range(1, 5):\n",
        "    test_dataset = MyDataset(ID_list, R_time_list=[fold_idx], model_list=model_list, flatten=True, model_type=0)\n",
        "    test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    x = fold_idx+1 if fold_idx+1<=5 else 1\n",
        "    val_dataset = MyDataset(ID_list, R_time_list=[x], model_list=model_list, flatten=True, model_type=0)\n",
        "    val_loader = Data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    R_time_list = []\n",
        "    for i in range(1, 9):\n",
        "      if i!=fold_idx and i!=x:\n",
        "        R_time_list.append(i)\n",
        "    train_dataset = MyDataset(ID_list, R_time_list=R_time_list, model_list=model_list, flatten=True, model_type=0)\n",
        "    train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "      \n",
        "    model_param=64\n",
        "    score[fold_idx-1] = main_function(total_epoch=300, dataloader_list=[train_loader, val_loader, test_loader], \\\n",
        "                                  model_para=model_param, model_save=False, cross_val=True)\n",
        "    print('fold{} Train Loss{}, Train Acc{}'.format(fold_idx, score[fold_idx-1, 0, 0], score[fold_idx-1, 0, 1]))\n",
        "    print('fold{} Val Loss{}, Val Acc{}'.format(fold_idx, score[fold_idx-1, 1, 0], score[fold_idx-1, 1, 1]))\n",
        "    print('fold{} Test Loss{}, Test Acc{}'.format(fold_idx, score[fold_idx-1, 2, 0], score[fold_idx-1, 2, 1]))\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  np.save(result_path + '/d0419_score_' + str(save_number), score)\n",
        "  save_number += 1\n",
        "\n",
        "  x = np.mean(score, axis=0)\n",
        "  print('Train', x[0])\n",
        "  print('Val', x[1])\n",
        "  print('Test', x[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrBOkZzjDAIk",
        "outputId": "780c42af-1f9f-4a10-a93f-45cd5647f0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:00<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold1 Train Loss0.002566661294126375, Train Acc1.0\n",
            "fold1 Val Loss0.28227162521216087, Val Acc0.9233333333333333\n",
            "fold1 Test Loss0.25943817313964246, Test Acc0.9255555555555556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:02<00:00,  2.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold2 Train Loss0.003434820746240968, Train Acc1.0\n",
            "fold2 Val Loss0.3122419146820903, Val Acc0.9211111111111111\n",
            "fold2 Test Loss0.29303138982504606, Test Acc0.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:00<00:00,  2.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold3 Train Loss0.0020078317876058545, Train Acc1.0\n",
            "fold3 Val Loss0.17481186788063496, Val Acc0.9454949944382648\n",
            "fold3 Test Loss0.19507323624566197, Test Acc0.9433333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:03<00:00,  2.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold4 Train Loss4.2872045690363105, Train Acc0.1376414749908726\n",
            "fold4 Val Loss4.398179709911346, Val Acc0.08240534521158129\n",
            "fold4 Test Loss4.541834056377411, Test Acc0.0778642936596218\n",
            "Train [1.07380347 0.78441037 0.78441037 0.77946734 0.78441037 0.78016692\n",
            " 0.78441037 0.78422494]\n",
            "Val [1.29187628 0.7180862  0.7180862  0.71477898 0.7180862  0.72387024\n",
            " 0.7180862  0.718     ]\n",
            "Test [1.32234421 0.7141883  0.7141883  0.70983562 0.7141883  0.71693156\n",
            " 0.7141883  0.71416667]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [01:59<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold1 Train Loss5.010537429289385, Train Acc0.007675438596491228\n",
            "fold1 Val Loss5.010453879833221, Val Acc0.006666666666666667\n",
            "fold1 Test Loss5.011168599128723, Test Acc0.006666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [01:59<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold2 Train Loss5.010097243569114, Train Acc0.004751461988304093\n",
            "fold2 Val Loss5.009352624416351, Val Acc0.006666666666666667\n",
            "fold2 Test Loss5.00787490606308, Test Acc0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [01:59<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold3 Train Loss5.010604338212446, Train Acc0.006576543660942638\n",
            "fold3 Val Loss5.009983003139496, Val Acc0.006674082313681869\n",
            "fold3 Test Loss5.010462462902069, Test Acc0.006666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:00<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold4 Train Loss5.0103866837241435, Train Acc0.007667031763417305\n",
            "fold4 Val Loss5.011408805847168, Val Acc0.0066815144766146995\n",
            "fold4 Test Loss5.011310040950775, Test Acc0.006674082313681869\n",
            "Train [5.01040642e+00 6.66761900e-03 6.66761900e-03 5.64062396e-04\n",
            " 6.66761900e-03 1.95903890e-03 6.66761900e-03 6.15824283e-03]\n",
            "Val [5.01029958e+00 6.67223253e-03 6.67223253e-03 2.39752393e-04\n",
            " 6.67223253e-03 1.27301096e-04 6.67223253e-03 6.66666667e-03]\n",
            "Test [5.01020400e+00 7.50185391e-03 7.50185391e-03 5.53670361e-04\n",
            " 7.50185391e-03 3.95435673e-04 7.50185391e-03 7.50000000e-03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "ID_list = [1,2,3,4,5,6]\n",
        "model_name_list = ['mlp', 'SVC']\n",
        "save_number = 14\n",
        "\n",
        "for m_idx in range(2):\n",
        "  model_list = []\n",
        "  for ID in range(1, 7):\n",
        "    model_name = result_path_isolated + '/'+ model_name_list[m_idx] + '_sub00' + str(ID) + '.pck'\n",
        "    model_stroke = pickle.load(open(model_name, \"rb\"))\n",
        "    model_list.append(model_stroke)\n",
        "\n",
        "  score = np.zeros((4, 3, 8))\n",
        "  for fold_idx in range(1, 5):\n",
        "    test_dataset = MyDataset(ID_list, R_time_list=[fold_idx], model_list=model_list, flatten=True, model_type=0)\n",
        "    test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    x = fold_idx+1 if fold_idx+1<=5 else 1\n",
        "    val_dataset = MyDataset(ID_list, R_time_list=[x], model_list=model_list, flatten=True, model_type=0)\n",
        "    val_loader = Data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    R_time_list = []\n",
        "    for i in range(1, 9):\n",
        "      if i!=fold_idx and i!=x:\n",
        "        R_time_list.append(i)\n",
        "    train_dataset = MyDataset(ID_list, R_time_list=R_time_list, model_list=model_list, flatten=True, model_type=0)\n",
        "    train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "      \n",
        "    model_param=64\n",
        "    score[fold_idx-1] = main_function(total_epoch=300, dataloader_list=[train_loader, val_loader, test_loader], \\\n",
        "                                  model_para=model_param, model_save=False, cross_val=True, weight_decay=2e-3)\n",
        "    print('fold{} Train Loss{}, Train Acc{}'.format(fold_idx, score[fold_idx-1, 0, 0], score[fold_idx-1, 0, 1]))\n",
        "    print('fold{} Val Loss{}, Val Acc{}'.format(fold_idx, score[fold_idx-1, 1, 0], score[fold_idx-1, 1, 1]))\n",
        "    print('fold{} Test Loss{}, Test Acc{}'.format(fold_idx, score[fold_idx-1, 2, 0], score[fold_idx-1, 2, 1]))\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  np.save(result_path + '/d0419_score_' + str(save_number), score)\n",
        "  save_number += 1\n",
        "\n",
        "  x = np.mean(score, axis=0)\n",
        "  print('Train', x[0])\n",
        "  print('Val', x[1])\n",
        "  print('Test', x[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uTaDKMxIAfd",
        "outputId": "7cb4a10c-a55a-440a-aa94-46c4c3f872a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [01:59<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold1 Train Loss5.009634082967585, Train Acc0.01023391812865497\n",
            "fold1 Val Loss5.010189235210419, Val Acc0.005555555555555556\n",
            "fold1 Test Loss5.010197579860687, Test Acc0.006666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:03<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold2 Train Loss4.320016557520086, Train Acc0.14839181286549707\n",
            "fold2 Val Loss4.552741646766663, Val Acc0.07888888888888888\n",
            "fold2 Test Loss4.483769714832306, Test Acc0.09444444444444444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:03<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold3 Train Loss4.415424737063321, Train Acc0.13737668980635734\n",
            "fold3 Val Loss4.601188004016876, Val Acc0.08342602892102335\n",
            "fold3 Test Loss4.615572869777679, Test Acc0.08888888888888889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:03<00:00,  2.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold4 Train Loss0.01669037820432674, Train Acc1.0\n",
            "fold4 Val Loss0.29843968921341, Val Acc0.9064587973273942\n",
            "fold4 Test Loss0.3514192735310644, Test Acc0.8887652947719689\n",
            "Train [3.44044144 0.32400061 0.32400061 0.3098     0.32400061 0.31730078\n",
            " 0.32400061 0.32324487]\n",
            "Val [3.61563964 0.26858232 0.26858232 0.25838729 0.26858232 0.26380371\n",
            " 0.26858232 0.26855556]\n",
            "Test [3.61523986 0.26969132 0.26969132 0.26078894 0.26969132 0.26913544\n",
            " 0.26969132 0.26972222]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:00<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold1 Train Loss5.010668776252053, Train Acc0.005847953216374269\n",
            "fold1 Val Loss5.010509729385376, Val Acc0.0033333333333333335\n",
            "fold1 Test Loss5.0103267431259155, Test Acc0.0044444444444444444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:00<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold2 Train Loss5.011329564181241, Train Acc0.006578947368421052\n",
            "fold2 Val Loss5.0086387395858765, Val Acc0.006666666666666667\n",
            "fold2 Test Loss5.012802600860596, Test Acc0.006666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:00<00:00,  2.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold3 Train Loss5.010767438194969, Train Acc0.006576543660942638\n",
            "fold3 Val Loss5.009313523769379, Val Acc0.006674082313681869\n",
            "fold3 Test Loss5.010397374629974, Test Acc0.006666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:01<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold4 Train Loss5.0111802057786425, Train Acc0.007667031763417305\n",
            "fold4 Val Loss5.009488344192505, Val Acc0.0066815144766146995\n",
            "fold4 Test Loss5.022032678127289, Test Acc0.006674082313681869\n",
            "Train [5.01098650e+00 6.66761900e-03 6.66761900e-03 9.29730824e-05\n",
            " 6.66761900e-03 4.68309289e-05 6.66761900e-03 6.40350877e-03]\n",
            "Val [5.00948758e+00 5.83889920e-03 5.83889920e-03 7.97936732e-05\n",
            " 5.83889920e-03 4.01731602e-05 5.83889920e-03 5.83333333e-03]\n",
            "Test [5.01388985e+00 6.11296502e-03 6.11296502e-03 8.44396443e-05\n",
            " 6.11296502e-03 4.25157982e-05 6.11296502e-03 6.11111111e-03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "ID_list = [1,2,3,4,5,6]\n",
        "model_name_list = ['mlp', 'SVC']\n",
        "save_number = 16\n",
        "\n",
        "for m_idx in range(2):\n",
        "  model_list = []\n",
        "  for ID in range(1, 7):\n",
        "    model_name = result_path_isolated + '/'+ model_name_list[m_idx] + '_sub00' + str(ID) + '.pck'\n",
        "    model_stroke = pickle.load(open(model_name, \"rb\"))\n",
        "    model_list.append(model_stroke)\n",
        "\n",
        "  score = np.zeros((4, 3, 8))\n",
        "  for fold_idx in range(1, 5):\n",
        "    test_dataset = MyDataset(ID_list, R_time_list=[fold_idx], model_list=model_list, flatten=True, model_type=0)\n",
        "    test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    x = fold_idx+1 if fold_idx+1<=5 else 1\n",
        "    val_dataset = MyDataset(ID_list, R_time_list=[x], model_list=model_list, flatten=True, model_type=0)\n",
        "    val_loader = Data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    R_time_list = []\n",
        "    for i in range(1, 9):\n",
        "      if i!=fold_idx and i!=x:\n",
        "        R_time_list.append(i)\n",
        "    train_dataset = MyDataset(ID_list, R_time_list=R_time_list, model_list=model_list, flatten=True, model_type=0)\n",
        "    train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "      \n",
        "    model_param=64\n",
        "    score[fold_idx-1] = main_function(total_epoch=300, dataloader_list=[train_loader, val_loader, test_loader], \\\n",
        "                                  model_para=model_param, model_save=False, cross_val=True, weight_decay=1e-4)\n",
        "    print('fold{} Train Loss{}, Train Acc{}'.format(fold_idx, score[fold_idx-1, 0, 0], score[fold_idx-1, 0, 1]))\n",
        "    print('fold{} Val Loss{}, Val Acc{}'.format(fold_idx, score[fold_idx-1, 1, 0], score[fold_idx-1, 1, 1]))\n",
        "    print('fold{} Test Loss{}, Test Acc{}'.format(fold_idx, score[fold_idx-1, 2, 0], score[fold_idx-1, 2, 1]))\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  np.save(result_path + '/d0419_score_' + str(save_number), score)\n",
        "  save_number += 1\n",
        "\n",
        "  x = np.mean(score, axis=0)\n",
        "  print('Train', x[0])\n",
        "  print('Val', x[1])\n",
        "  print('Test', x[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYBgE0eKMnVC",
        "outputId": "6ee61e5e-354f-4789-d1bb-2f4d96a901d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:03<00:00,  2.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold1 Train Loss0.0005534725866957822, Train Acc1.0\n",
            "fold1 Val Loss0.2497770295594819, Val Acc0.9288888888888889\n",
            "fold1 Test Loss0.2479567216223586, Test Acc0.9277777777777778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:03<00:00,  2.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold2 Train Loss0.0005557983968174085, Train Acc1.0\n",
            "fold2 Val Loss0.24993076687678695, Val Acc0.9388888888888889\n",
            "fold2 Test Loss0.2433132279838901, Test Acc0.9333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:03<00:00,  2.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold3 Train Loss0.00042930677723647517, Train Acc1.0\n",
            "fold3 Val Loss0.20610205573029816, Val Acc0.9388209121245829\n",
            "fold3 Test Loss0.23833313724026084, Test Acc0.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:04<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold4 Train Loss0.000217764387013052, Train Acc1.0\n",
            "fold4 Val Loss0.16673591715516523, Val Acc0.9409799554565702\n",
            "fold4 Test Loss0.23424467735458165, Test Acc0.9421579532814238\n",
            "Train [4.39085537e-04 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
            "Val [0.21813644 0.93689466 0.93689466 0.93644451 0.93689466 0.94527579\n",
            " 0.93689466 0.93683333]\n",
            "Test [0.24096194 0.93581727 0.93581727 0.93581518 0.93581727 0.94495503\n",
            " 0.93581727 0.93583333]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:03<00:00,  2.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold1 Train Loss0.0009641268539284779, Train Acc1.0\n",
            "fold1 Val Loss0.26378714853490237, Val Acc0.9255555555555556\n",
            "fold1 Test Loss0.26019706301121914, Test Acc0.9255555555555556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:02<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold2 Train Loss0.0003199148208791898, Train Acc1.0\n",
            "fold2 Val Loss0.28129118151264265, Val Acc0.93\n",
            "fold2 Test Loss0.24922210826480296, Test Acc0.9288888888888889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:03<00:00,  2.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold3 Train Loss0.00029737329499436203, Train Acc1.0\n",
            "fold3 Val Loss0.24535674609069247, Val Acc0.9343715239154616\n",
            "fold3 Test Loss0.25852770069286635, Test Acc0.9433333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:04<00:00,  2.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold4 Train Loss0.00018733346157453278, Train Acc1.0\n",
            "fold4 Val Loss0.20672691779100205, Val Acc0.9409799554565702\n",
            "fold4 Test Loss0.22770134952588705, Test Acc0.9454949944382648\n",
            "Train [4.42187108e-04 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
            "Val [0.2492905  0.93272676 0.93272676 0.93202133 0.93272676 0.94087302\n",
            " 0.93272676 0.93272222]\n",
            "Test [0.24891206 0.93581819 0.93581819 0.93532649 0.93581819 0.94445274\n",
            " 0.93581819 0.93583333]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "ID_list = [1,2,3,4,5,6]\n",
        "model_name_list = ['mlp', 'SVC']\n",
        "save_number = 18\n",
        "\n",
        "for m_idx in range(2):\n",
        "  model_list = []\n",
        "  for ID in range(1, 7):\n",
        "    model_name = result_path_isolated + '/'+ model_name_list[m_idx] + '_sub00' + str(ID) + '.pck'\n",
        "    model_stroke = pickle.load(open(model_name, \"rb\"))\n",
        "    model_list.append(model_stroke)\n",
        "\n",
        "  score = np.zeros((4, 3, 8))\n",
        "  for fold_idx in range(1, 5):\n",
        "    test_dataset = MyDataset(ID_list, R_time_list=[fold_idx], model_list=model_list, flatten=True, model_type=0)\n",
        "    test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    x = fold_idx+1 if fold_idx+1<=5 else 1\n",
        "    val_dataset = MyDataset(ID_list, R_time_list=[x], model_list=model_list, flatten=True, model_type=0)\n",
        "    val_loader = Data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    R_time_list = []\n",
        "    for i in range(1, 9):\n",
        "      if i!=fold_idx and i!=x:\n",
        "        R_time_list.append(i)\n",
        "    train_dataset = MyDataset(ID_list, R_time_list=R_time_list, model_list=model_list, flatten=True, model_type=0)\n",
        "    train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "      \n",
        "    model_param=64\n",
        "    score[fold_idx-1] = main_function(total_epoch=300, dataloader_list=[train_loader, val_loader, test_loader], \\\n",
        "                                  model_para=model_param, model_save=False, cross_val=True, weight_decay=1e-4)\n",
        "    print('fold{} Train Loss{}, Train Acc{}'.format(fold_idx, score[fold_idx-1, 0, 0], score[fold_idx-1, 0, 1]))\n",
        "    print('fold{} Val Loss{}, Val Acc{}'.format(fold_idx, score[fold_idx-1, 1, 0], score[fold_idx-1, 1, 1]))\n",
        "    print('fold{} Test Loss{}, Test Acc{}'.format(fold_idx, score[fold_idx-1, 2, 0], score[fold_idx-1, 2, 1]))\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  np.save(result_path + '/d0419_score_' + str(save_number), score)\n",
        "  save_number += 1\n",
        "\n",
        "  x = np.mean(score, axis=0)\n",
        "  print('Train', x[0])\n",
        "  print('Val', x[1])\n",
        "  print('Test', x[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN7XRoQ5RYFx",
        "outputId": "63f9001a-4385-441a-b518-3a3025026c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:03<00:00,  2.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold1 Train Loss0.0005034695244500075, Train Acc1.0\n",
            "fold1 Val Loss0.2613779172716022, Val Acc0.9344444444444444\n",
            "fold1 Test Loss0.24747333677805727, Test Acc0.9255555555555556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:03<00:00,  2.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold2 Train Loss0.00042458974878007376, Train Acc1.0\n",
            "fold2 Val Loss0.26338601240422577, Val Acc0.9366666666666666\n",
            "fold2 Test Loss0.22460568789028912, Test Acc0.9377777777777778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:05<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold3 Train Loss0.00047237970151896167, Train Acc1.0\n",
            "fold3 Val Loss0.23245773956296034, Val Acc0.9299221357063404\n",
            "fold3 Test Loss0.2872425951063633, Test Acc0.9288888888888889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:06<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold4 Train Loss0.0006161587503315373, Train Acc1.0\n",
            "fold4 Val Loss0.21083738070592517, Val Acc0.9398663697104677\n",
            "fold4 Test Loss0.26072595082223415, Test Acc0.9265850945494994\n",
            "Train [5.04149431e-04 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
            "Val [0.24201476 0.9352249  0.9352249  0.93465675 0.9352249  0.94332143\n",
            " 0.9352249  0.93522222]\n",
            "Test [0.25501189 0.92970183 0.92970183 0.92924753 0.92970183 0.93826984\n",
            " 0.92970183 0.92972222]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:05<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold1 Train Loss0.000505782706816469, Train Acc1.0\n",
            "fold1 Val Loss0.30798001607035985, Val Acc0.9244444444444444\n",
            "fold1 Test Loss0.2225286560405948, Test Acc0.9355555555555556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:04<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold2 Train Loss0.0005160212762844326, Train Acc1.0\n",
            "fold2 Val Loss0.2967454297468066, Val Acc0.93\n",
            "fold2 Test Loss0.24449375856193, Test Acc0.9344444444444444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:04<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold3 Train Loss0.0005181974636136808, Train Acc1.0\n",
            "fold3 Val Loss0.24342807995708426, Val Acc0.9310344827586207\n",
            "fold3 Test Loss0.3274388900026679, Test Acc0.9277777777777778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:05<00:00,  2.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold4 Train Loss0.0004921906035054814, Train Acc1.0\n",
            "fold4 Val Loss0.23523471796397644, Val Acc0.9320712694877505\n",
            "fold4 Test Loss0.25342347437981516, Test Acc0.9310344827586207\n",
            "Train [5.08048013e-04 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
            "Val [0.27084706 0.92938755 0.92938755 0.92880427 0.92938755 0.93819048\n",
            " 0.92938755 0.92938889]\n",
            "Test [0.26197119 0.93220307 0.93220307 0.93196306 0.93220307 0.94118056\n",
            " 0.93220307 0.93216667]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***d0420***"
      ],
      "metadata": {
        "id": "PS6E_-SBlix5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leave one subject out"
      ],
      "metadata": {
        "id": "V9OhRXC9drf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "R_time_list = [1,2,3,4,5,6, 7, 8]\n",
        "model_name_list = ['mlp', 'SVC']\n",
        "save_number = 0\n",
        "\n",
        "for m_idx in range(2):\n",
        "  # Load model\n",
        "  model_list = []\n",
        "  for ID in range(1, 7):\n",
        "    model_name = result_path_isolated + '/'+ model_name_list[m_idx] + '_sub00' + str(ID) + '.pck'\n",
        "    model_stroke = pickle.load(open(model_name, \"rb\"))\n",
        "    model_list.append(model_stroke)\n",
        "\n",
        "  # Leave subject out\n",
        "  score = np.zeros((6, 3, 8))\n",
        "  for ID in range(1, 7):\n",
        "    test_dataset = MyDataset([ID], R_time_list=R_time_list, model_list=[model_list[ID-1]], flatten=True, model_type=0)\n",
        "    test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    x = ID+1 if ID+1<=6 else 1\n",
        "    val_dataset = MyDataset([x], R_time_list=R_time_list, model_list=[model_list[x-1]], flatten=True, model_type=0)\n",
        "    val_loader = Data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    Model_list_train = []\n",
        "    ID_list_train = []\n",
        "    for i in range(1, 7):\n",
        "      if i!=ID and i!=x:\n",
        "        Model_list_train.append(model_list[i-1])\n",
        "        ID_list_train.append(i)\n",
        "    train_dataset = MyDataset(ID_list_train, R_time_list=R_time_list, model_list=Model_list_train, flatten=True, model_type=0)\n",
        "    train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "      \n",
        "    model_param=64\n",
        "    score[ID-1] = main_function(total_epoch=300, dataloader_list=[train_loader, val_loader, test_loader], \\\n",
        "                                  model_para=model_param, model_save=False, cross_val=True, weight_decay=1e-4)\n",
        "    print('ID{} Train Loss{}, Train Acc{}'.format(ID, score[ID-1, 0, 0], score[ID-1, 0, 1]))\n",
        "    print('ID{} Val Loss{}, Val Acc{}'.format(ID, score[ID-1, 1, 0], score[ID-1, 1, 1]))\n",
        "    print('ID{} Test Loss{}, Test Acc{}'.format(ID, score[ID-1, 2, 0], score[ID-1, 2, 1]))\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  np.save(result_path + '/d0420_score_' + str(save_number), score)\n",
        "  save_number += 1\n",
        "\n",
        "  x = np.mean(score, axis=0)\n",
        "  print('Train', x[0])\n",
        "  print('Val', x[1])\n",
        "  print('Test', x[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN_DC2a2dyqD",
        "outputId": "789e3099-fcb2-4034-96cb-d720e57967fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/300 [00:08<40:11,  8.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:0    Train set: Average loss: 5.0077\n",
            "EPOCH:0    Validation set: Average loss: 5.0103\n",
            "EPOCH:0    Test set: Average loss: 5.0119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 51/300 [00:27<01:35,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:50    Train set: Average loss: 0.0023\n",
            "EPOCH:50    Validation set: Average loss: 3.0914\n",
            "EPOCH:50    Test set: Average loss: 5.6473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 101/300 [00:46<01:16,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:100    Train set: Average loss: 0.0006\n",
            "EPOCH:100    Validation set: Average loss: 3.1715\n",
            "EPOCH:100    Test set: Average loss: 5.9401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 151/300 [01:05<00:57,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:150    Train set: Average loss: 0.0003\n",
            "EPOCH:150    Validation set: Average loss: 3.1892\n",
            "EPOCH:150    Test set: Average loss: 5.9362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 201/300 [01:24<00:37,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:200    Train set: Average loss: 0.0002\n",
            "EPOCH:200    Validation set: Average loss: 3.1507\n",
            "EPOCH:200    Test set: Average loss: 5.9471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 251/300 [01:43<00:18,  2.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:250    Train set: Average loss: 0.0002\n",
            "EPOCH:250    Validation set: Average loss: 3.2364\n",
            "EPOCH:250    Test set: Average loss: 5.9110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:02<00:00,  2.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID1 Train Loss0.09836178754145901, Train Acc0.985836627140975\n",
            "ID1 Val Loss2.6924230655034385, Val Acc0.47333333333333333\n",
            "ID1 Test Loss4.746031284332275, Test Acc0.144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/300 [00:00<02:16,  2.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:0    Train set: Average loss: 5.0096\n",
            "EPOCH:0    Validation set: Average loss: 5.0109\n",
            "EPOCH:0    Test set: Average loss: 5.0106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 51/300 [00:20<01:39,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:50    Train set: Average loss: 0.0024\n",
            "EPOCH:50    Validation set: Average loss: 2.7633\n",
            "EPOCH:50    Test set: Average loss: 2.6645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 101/300 [00:40<01:19,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:100    Train set: Average loss: 0.0006\n",
            "EPOCH:100    Validation set: Average loss: 2.8422\n",
            "EPOCH:100    Test set: Average loss: 2.7095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 151/300 [00:59<00:59,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:150    Train set: Average loss: 0.0003\n",
            "EPOCH:150    Validation set: Average loss: 2.8624\n",
            "EPOCH:150    Test set: Average loss: 2.6690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 201/300 [01:19<00:39,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:200    Train set: Average loss: 0.0003\n",
            "EPOCH:200    Validation set: Average loss: 2.8222\n",
            "EPOCH:200    Test set: Average loss: 2.6150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 251/300 [01:40<00:19,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:250    Train set: Average loss: 0.0002\n",
            "EPOCH:250    Validation set: Average loss: 2.8061\n",
            "EPOCH:250    Test set: Average loss: 2.6121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [01:59<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID2 Train Loss0.07917270204052329, Train Acc0.9933993399339934\n",
            "ID2 Val Loss2.4816970427831015, Val Acc0.47354497354497355\n",
            "ID2 Test Loss2.3577187856038413, Test Acc0.49333333333333335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/300 [00:00<02:17,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:0    Train set: Average loss: 5.0033\n",
            "EPOCH:0    Validation set: Average loss: 5.0093\n",
            "EPOCH:0    Test set: Average loss: 5.0083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 51/300 [00:20<01:38,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:50    Train set: Average loss: 0.0034\n",
            "EPOCH:50    Validation set: Average loss: 3.8740\n",
            "EPOCH:50    Test set: Average loss: 2.0132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 101/300 [00:39<01:17,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:100    Train set: Average loss: 0.0008\n",
            "EPOCH:100    Validation set: Average loss: 4.0534\n",
            "EPOCH:100    Test set: Average loss: 2.0257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 151/300 [00:59<00:58,  2.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:150    Train set: Average loss: 0.0005\n",
            "EPOCH:150    Validation set: Average loss: 4.0732\n",
            "EPOCH:150    Test set: Average loss: 1.9555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 201/300 [01:19<00:40,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:200    Train set: Average loss: 0.0004\n",
            "EPOCH:200    Validation set: Average loss: 3.9051\n",
            "EPOCH:200    Test set: Average loss: 1.9250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 251/300 [01:39<00:19,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:250    Train set: Average loss: 0.0003\n",
            "EPOCH:250    Validation set: Average loss: 3.9072\n",
            "EPOCH:250    Test set: Average loss: 1.9377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [01:59<00:00,  2.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID3 Train Loss0.2376789196083943, Train Acc0.9669093315684977\n",
            "ID3 Val Loss3.439234654108683, Val Acc0.2704485488126649\n",
            "ID3 Test Loss2.177949825922648, Test Acc0.5066137566137566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/300 [00:00<02:16,  2.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:0    Train set: Average loss: 5.0055\n",
            "EPOCH:0    Validation set: Average loss: 5.0085\n",
            "EPOCH:0    Test set: Average loss: 5.0099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 51/300 [00:20<01:40,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:50    Train set: Average loss: 0.0042\n",
            "EPOCH:50    Validation set: Average loss: 1.3988\n",
            "EPOCH:50    Test set: Average loss: 4.1927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 101/300 [00:40<01:18,  2.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:100    Train set: Average loss: 0.0011\n",
            "EPOCH:100    Validation set: Average loss: 1.3813\n",
            "EPOCH:100    Test set: Average loss: 4.2274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 151/300 [01:00<00:59,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:150    Train set: Average loss: 0.0006\n",
            "EPOCH:150    Validation set: Average loss: 1.3189\n",
            "EPOCH:150    Test set: Average loss: 4.3408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 201/300 [01:20<00:38,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:200    Train set: Average loss: 0.0004\n",
            "EPOCH:200    Validation set: Average loss: 1.3119\n",
            "EPOCH:200    Test set: Average loss: 4.4071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 251/300 [01:40<00:19,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:250    Train set: Average loss: 0.0004\n",
            "EPOCH:250    Validation set: Average loss: 1.3396\n",
            "EPOCH:250    Test set: Average loss: 4.3811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:00<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID4 Train Loss0.0005281818788110589, Train Acc1.0\n",
            "ID4 Val Loss1.3003738820552826, Val Acc0.7463863337713534\n",
            "ID4 Test Loss4.6320226192474365, Test Acc0.32849604221635886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/300 [00:00<02:14,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:0    Train set: Average loss: 5.0080\n",
            "EPOCH:0    Validation set: Average loss: 5.0116\n",
            "EPOCH:0    Test set: Average loss: 5.0099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 51/300 [00:20<01:38,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:50    Train set: Average loss: 0.0041\n",
            "EPOCH:50    Validation set: Average loss: 1.4443\n",
            "EPOCH:50    Test set: Average loss: 1.9802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 101/300 [00:40<01:21,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:100    Train set: Average loss: 0.0011\n",
            "EPOCH:100    Validation set: Average loss: 1.3543\n",
            "EPOCH:100    Test set: Average loss: 1.9961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 151/300 [01:00<00:59,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:150    Train set: Average loss: 0.0006\n",
            "EPOCH:150    Validation set: Average loss: 1.3261\n",
            "EPOCH:150    Test set: Average loss: 2.0081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 201/300 [01:20<00:39,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:200    Train set: Average loss: 0.0005\n",
            "EPOCH:200    Validation set: Average loss: 1.2815\n",
            "EPOCH:200    Test set: Average loss: 1.9807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 251/300 [01:39<00:19,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:250    Train set: Average loss: 0.0004\n",
            "EPOCH:250    Validation set: Average loss: 1.2803\n",
            "EPOCH:250    Test set: Average loss: 1.9690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [01:59<00:00,  2.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID5 Train Loss0.00037977122944236424, Train Acc1.0\n",
            "ID5 Val Loss1.2643595337867737, Val Acc0.7174770039421814\n",
            "ID5 Test Loss1.9811695615450542, Test Acc0.6386333771353482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/300 [00:00<02:11,  2.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:0    Train set: Average loss: 5.0049\n",
            "EPOCH:0    Validation set: Average loss: 5.0110\n",
            "EPOCH:0    Test set: Average loss: 5.0092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 51/300 [00:20<01:39,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:50    Train set: Average loss: 0.0036\n",
            "EPOCH:50    Validation set: Average loss: 4.3643\n",
            "EPOCH:50    Test set: Average loss: 0.6459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 101/300 [00:40<01:20,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:100    Train set: Average loss: 0.0009\n",
            "EPOCH:100    Validation set: Average loss: 4.2124\n",
            "EPOCH:100    Test set: Average loss: 0.6790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 151/300 [01:01<01:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:150    Train set: Average loss: 0.0005\n",
            "EPOCH:150    Validation set: Average loss: 4.2859\n",
            "EPOCH:150    Test set: Average loss: 0.6718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 201/300 [01:21<00:40,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:200    Train set: Average loss: 0.0003\n",
            "EPOCH:200    Validation set: Average loss: 4.3687\n",
            "EPOCH:200    Test set: Average loss: 0.6471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 251/300 [01:41<00:19,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:250    Train set: Average loss: 0.0003\n",
            "EPOCH:250    Validation set: Average loss: 4.3520\n",
            "EPOCH:250    Test set: Average loss: 0.6491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:01<00:00,  2.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID6 Train Loss0.28083800338208675, Train Acc0.9500826446280992\n",
            "ID6 Val Loss3.6545166969299316, Val Acc0.22133333333333333\n",
            "ID6 Test Loss1.0857292513052623, Test Acc0.7122207621550591\n",
            "Train [0.11615989 0.98270466 0.98270466 0.98264725 0.98270466 0.9834978\n",
            " 0.98270466 0.98274029]\n",
            "Val [2.47210081 0.48375392 0.48375392 0.45603852 0.48375392 0.49764326\n",
            " 0.48375392 0.4850172 ]\n",
            "Test [2.83010355 0.47054955 0.47054955 0.43560465 0.47054955 0.46504796\n",
            " 0.47054955 0.47093651]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/300 [00:00<02:04,  2.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:0    Train set: Average loss: 5.0093\n",
            "EPOCH:0    Validation set: Average loss: 5.0106\n",
            "EPOCH:0    Test set: Average loss: 5.0103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 51/300 [00:20<01:38,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:50    Train set: Average loss: 0.0031\n",
            "EPOCH:50    Validation set: Average loss: 3.1049\n",
            "EPOCH:50    Test set: Average loss: 6.3883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 101/300 [00:40<01:18,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:100    Train set: Average loss: 0.0009\n",
            "EPOCH:100    Validation set: Average loss: 3.4121\n",
            "EPOCH:100    Test set: Average loss: 6.6801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 151/300 [01:00<01:00,  2.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:150    Train set: Average loss: 0.0005\n",
            "EPOCH:150    Validation set: Average loss: 3.4863\n",
            "EPOCH:150    Test set: Average loss: 6.8298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 201/300 [01:20<00:40,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:200    Train set: Average loss: 0.0004\n",
            "EPOCH:200    Validation set: Average loss: 3.3684\n",
            "EPOCH:200    Test set: Average loss: 6.7052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 251/300 [01:40<00:19,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:250    Train set: Average loss: 0.0003\n",
            "EPOCH:250    Validation set: Average loss: 3.4008\n",
            "EPOCH:250    Test set: Average loss: 6.8981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [01:59<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID1 Train Loss0.14129464980214834, Train Acc0.9782608695652174\n",
            "ID1 Val Loss2.6966496308644614, Val Acc0.4573333333333333\n",
            "ID1 Test Loss5.274702390034993, Test Acc0.136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/300 [00:00<02:02,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:0    Train set: Average loss: 5.0102\n",
            "EPOCH:0    Validation set: Average loss: 5.0105\n",
            "EPOCH:0    Test set: Average loss: 5.0106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 51/300 [00:20<01:39,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:50    Train set: Average loss: 0.0029\n",
            "EPOCH:50    Validation set: Average loss: 2.9420\n",
            "EPOCH:50    Test set: Average loss: 3.1833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 101/300 [00:40<01:20,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:100    Train set: Average loss: 0.0008\n",
            "EPOCH:100    Validation set: Average loss: 2.8773\n",
            "EPOCH:100    Test set: Average loss: 3.1114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 151/300 [01:00<00:59,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:150    Train set: Average loss: 0.0004\n",
            "EPOCH:150    Validation set: Average loss: 2.8125\n",
            "EPOCH:150    Test set: Average loss: 3.1046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 201/300 [01:20<00:39,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:200    Train set: Average loss: 0.0003\n",
            "EPOCH:200    Validation set: Average loss: 2.9178\n",
            "EPOCH:200    Test set: Average loss: 3.1754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 251/300 [01:40<00:19,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:250    Train set: Average loss: 0.0002\n",
            "EPOCH:250    Validation set: Average loss: 2.9380\n",
            "EPOCH:250    Test set: Average loss: 3.1894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:00<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID2 Train Loss0.048872141322741904, Train Acc0.9963696369636964\n",
            "ID2 Val Loss2.576587955156962, Val Acc0.503968253968254\n",
            "ID2 Test Loss2.7078291972478232, Test Acc0.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/300 [00:00<02:08,  2.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:0    Train set: Average loss: 5.0053\n",
            "EPOCH:0    Validation set: Average loss: 5.0058\n",
            "EPOCH:0    Test set: Average loss: 5.0073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 51/300 [00:20<01:39,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:50    Train set: Average loss: 0.0023\n",
            "EPOCH:50    Validation set: Average loss: 4.5431\n",
            "EPOCH:50    Test set: Average loss: 2.0831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 101/300 [00:40<01:19,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:100    Train set: Average loss: 0.0006\n",
            "EPOCH:100    Validation set: Average loss: 4.7152\n",
            "EPOCH:100    Test set: Average loss: 2.1178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 151/300 [01:00<00:58,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:150    Train set: Average loss: 0.0003\n",
            "EPOCH:150    Validation set: Average loss: 4.7465\n",
            "EPOCH:150    Test set: Average loss: 2.1440\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 201/300 [01:19<00:39,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:200    Train set: Average loss: 0.0002\n",
            "EPOCH:200    Validation set: Average loss: 4.8580\n",
            "EPOCH:200    Test set: Average loss: 2.1151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 251/300 [01:39<00:19,  2.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:250    Train set: Average loss: 0.0002\n",
            "EPOCH:250    Validation set: Average loss: 4.6925\n",
            "EPOCH:250    Test set: Average loss: 2.0828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [01:59<00:00,  2.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID3 Train Loss0.10379925711701314, Train Acc0.985771012574454\n",
            "ID3 Val Loss3.892873485883077, Val Acc0.28232189973614774\n",
            "ID3 Test Loss1.9749664465586345, Test Acc0.5648148148148148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/300 [00:00<02:05,  2.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:0    Train set: Average loss: 5.0110\n",
            "EPOCH:0    Validation set: Average loss: 5.0110\n",
            "EPOCH:0    Test set: Average loss: 5.0110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 51/300 [00:20<01:39,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:50    Train set: Average loss: 0.0041\n",
            "EPOCH:50    Validation set: Average loss: 1.5470\n",
            "EPOCH:50    Test set: Average loss: 4.2990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 101/300 [00:40<01:19,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:100    Train set: Average loss: 0.0009\n",
            "EPOCH:100    Validation set: Average loss: 1.5221\n",
            "EPOCH:100    Test set: Average loss: 4.4637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 151/300 [01:00<01:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:150    Train set: Average loss: 0.0006\n",
            "EPOCH:150    Validation set: Average loss: 1.5331\n",
            "EPOCH:150    Test set: Average loss: 4.5748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 201/300 [01:20<00:40,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:200    Train set: Average loss: 0.0004\n",
            "EPOCH:200    Validation set: Average loss: 1.5118\n",
            "EPOCH:200    Test set: Average loss: 4.6544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 251/300 [01:40<00:19,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:250    Train set: Average loss: 0.0003\n",
            "EPOCH:250    Validation set: Average loss: 1.5555\n",
            "EPOCH:250    Test set: Average loss: 4.6368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:00<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID4 Train Loss0.0011294789292151108, Train Acc1.0\n",
            "ID4 Val Loss1.474111298720042, Val Acc0.7109067017082786\n",
            "ID4 Test Loss4.547942956288655, Test Acc0.32189973614775724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/300 [00:00<02:02,  2.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:0    Train set: Average loss: 5.0099\n",
            "EPOCH:0    Validation set: Average loss: 5.0107\n",
            "EPOCH:0    Test set: Average loss: 5.0109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 51/300 [00:20<01:38,  2.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:50    Train set: Average loss: 0.0071\n",
            "EPOCH:50    Validation set: Average loss: 1.6221\n",
            "EPOCH:50    Test set: Average loss: 2.3265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 101/300 [00:40<01:22,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:100    Train set: Average loss: 0.0015\n",
            "EPOCH:100    Validation set: Average loss: 1.4895\n",
            "EPOCH:100    Test set: Average loss: 2.4343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 151/300 [01:00<01:01,  2.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:150    Train set: Average loss: 0.0009\n",
            "EPOCH:150    Validation set: Average loss: 1.4824\n",
            "EPOCH:150    Test set: Average loss: 2.4002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 201/300 [01:20<00:39,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:200    Train set: Average loss: 0.0006\n",
            "EPOCH:200    Validation set: Average loss: 1.4576\n",
            "EPOCH:200    Test set: Average loss: 2.3232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 251/300 [01:41<00:19,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:250    Train set: Average loss: 0.0006\n",
            "EPOCH:250    Validation set: Average loss: 1.3967\n",
            "EPOCH:250    Test set: Average loss: 2.2795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:00<00:00,  2.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID5 Train Loss0.0010028130394251396, Train Acc1.0\n",
            "ID5 Val Loss1.3793072203795116, Val Acc0.7069645203679369\n",
            "ID5 Test Loss2.3736093839009604, Test Acc0.5755584756898817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/300 [00:00<02:03,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:0    Train set: Average loss: 5.0106\n",
            "EPOCH:0    Validation set: Average loss: 5.0107\n",
            "EPOCH:0    Test set: Average loss: 5.0109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 51/300 [00:20<01:39,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:50    Train set: Average loss: 0.0051\n",
            "EPOCH:50    Validation set: Average loss: 4.1699\n",
            "EPOCH:50    Test set: Average loss: 0.7325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▎      | 101/300 [00:40<01:19,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:100    Train set: Average loss: 0.0011\n",
            "EPOCH:100    Validation set: Average loss: 4.3889\n",
            "EPOCH:100    Test set: Average loss: 0.6912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 151/300 [01:00<01:00,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:150    Train set: Average loss: 0.0007\n",
            "EPOCH:150    Validation set: Average loss: 4.3730\n",
            "EPOCH:150    Test set: Average loss: 0.6636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 201/300 [01:21<00:39,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:200    Train set: Average loss: 0.0004\n",
            "EPOCH:200    Validation set: Average loss: 4.5393\n",
            "EPOCH:200    Test set: Average loss: 0.7107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 251/300 [01:41<00:19,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:250    Train set: Average loss: 0.0004\n",
            "EPOCH:250    Validation set: Average loss: 4.3788\n",
            "EPOCH:250    Test set: Average loss: 0.6867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:01<00:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID6 Train Loss0.07251232365767162, Train Acc0.9937190082644628\n",
            "ID6 Val Loss3.8100925286610923, Val Acc0.288\n",
            "ID6 Test Loss0.8774158159891764, Test Acc0.7634691195795007\n",
            "Train [0.06143511 0.99235342 0.99235342 0.99236854 0.99235342 0.99268854\n",
            " 0.99235342 0.99239392]\n",
            "Val [2.63827035 0.49158245 0.49158245 0.46025467 0.49158245 0.50088573\n",
            " 0.49158245 0.49302513]\n",
            "Test [2.95941103 0.46695702 0.46695702 0.43629489 0.46695702 0.47966021\n",
            " 0.46695702 0.46726984]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "R_time_list = [1,2,3,4,5,6, 7, 8]\n",
        "model_name_list = ['mlp', 'SVC']\n",
        "save_number = 2\n",
        "\n",
        "for m_idx in range(2):\n",
        "  # Load model\n",
        "  model_list = []\n",
        "  for ID in range(1, 7):\n",
        "    model_name = result_path_isolated + '/'+ model_name_list[m_idx] + '_sub00' + str(ID) + '.pck'\n",
        "    model_stroke = pickle.load(open(model_name, \"rb\"))\n",
        "    model_list.append(model_stroke)\n",
        "\n",
        "  # Leave subject out\n",
        "  score = np.zeros((6, 3, 8))\n",
        "  for ID in range(1, 7):\n",
        "    test_dataset = MyDataset([ID], R_time_list=R_time_list, model_list=[model_list[ID-1]], flatten=True, model_type=0)\n",
        "    test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    x = ID+1 if ID+1<=6 else 1\n",
        "    val_dataset = MyDataset([x], R_time_list=R_time_list, model_list=[model_list[x-1]], flatten=True, model_type=0)\n",
        "    val_loader = Data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    Model_list_train = []\n",
        "    ID_list_train = []\n",
        "    for i in range(1, 7):\n",
        "      if i!=ID and i!=x:\n",
        "        Model_list_train.append(model_list[i-1])\n",
        "        ID_list_train.append(i)\n",
        "    train_dataset = MyDataset(ID_list_train, R_time_list=R_time_list, model_list=Model_list_train, flatten=True, model_type=0)\n",
        "    train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "      \n",
        "    model_param=64\n",
        "    score[ID-1] = main_function(total_epoch=200, dataloader_list=[train_loader, val_loader, test_loader], \\\n",
        "                                  model_para=model_param, model_save=False, cross_val=True, weight_decay=2e-3)\n",
        "    print('ID{} Train Loss{}, Train Acc{}'.format(ID, score[ID-1, 0, 0], score[ID-1, 0, 1]))\n",
        "    print('ID{} Val Loss{}, Val Acc{}'.format(ID, score[ID-1, 1, 0], score[ID-1, 1, 1]))\n",
        "    print('ID{} Test Loss{}, Test Acc{}'.format(ID, score[ID-1, 2, 0], score[ID-1, 2, 1]))\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  np.save(result_path + '/d0420_score_' + str(save_number), score)\n",
        "  save_number += 1\n",
        "\n",
        "  x = np.mean(score, axis=0)\n",
        "  print('Train', x[0])\n",
        "  print('Val', x[1])\n",
        "  print('Test', x[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3J95SxTme-H",
        "outputId": "aea5d56f-a32a-4980-826f-0b9d6d8e9925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:04<00:00,  3.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID1 Train Loss0.01182405836880207, Train Acc1.0\n",
            "ID1 Val Loss2.618264675140381, Val Acc0.504\n",
            "ID1 Test Loss4.607635736465454, Test Acc0.192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:04<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID2 Train Loss0.013154251345743736, Train Acc1.0\n",
            "ID2 Val Loss2.1136680841445923, Val Acc0.5423280423280423\n",
            "ID2 Test Loss1.8561499118804932, Test Acc0.552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:04<00:00,  3.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID3 Train Loss0.013147923164069653, Train Acc1.0\n",
            "ID3 Val Loss3.4115182161331177, Val Acc0.32058047493403696\n",
            "ID3 Test Loss1.8136253356933594, Test Acc0.6150793650793651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:02<00:00,  3.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID4 Train Loss0.02438491676002741, Train Acc1.0\n",
            "ID4 Val Loss1.5513753294944763, Val Acc0.6557161629434954\n",
            "ID4 Test Loss4.073800563812256, Test Acc0.2770448548812665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:03<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID5 Train Loss0.031643337570130825, Train Acc0.9993364299933643\n",
            "ID5 Val Loss1.5523433685302734, Val Acc0.6281208935611038\n",
            "ID5 Test Loss2.355022430419922, Test Acc0.5519053876478318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:03<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID6 Train Loss0.023082768234113853, Train Acc0.9996694214876033\n",
            "ID6 Val Loss3.978737235069275, Val Acc0.24133333333333334\n",
            "ID6 Test Loss0.731559157371521, Test Acc0.8015768725361366\n",
            "Train [0.01953954 0.99983431 0.99983431 0.99984083 0.99983431 0.99984127\n",
            " 0.99983431 0.99984783]\n",
            "Val [2.53765115 0.48201315 0.48201315 0.45335757 0.48201315 0.49798739\n",
            " 0.48201315 0.48311111]\n",
            "Test [2.57296552 0.49826775 0.49826775 0.47193129 0.49826775 0.51427306\n",
            " 0.49826775 0.49919048]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:04<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID1 Train Loss0.017329192254692316, Train Acc0.9996706192358367\n",
            "ID1 Val Loss3.1242681741714478, Val Acc0.45866666666666667\n",
            "ID1 Test Loss5.664059400558472, Test Acc0.144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:03<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID2 Train Loss0.023375179308156174, Train Acc0.9993399339933994\n",
            "ID2 Val Loss3.3726072311401367, Val Acc0.3862433862433862\n",
            "ID2 Test Loss3.123603343963623, Test Acc0.392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:03<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID3 Train Loss3.471134583155314, Train Acc0.2872270019854401\n",
            "ID3 Val Loss4.4157891273498535, Val Acc0.08179419525065963\n",
            "ID3 Test Loss4.017323136329651, Test Acc0.12962962962962962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:03<00:00,  3.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID4 Train Loss0.02242873205492894, Train Acc1.0\n",
            "ID4 Val Loss1.7180609107017517, Val Acc0.6478318002628121\n",
            "ID4 Test Loss4.1731343269348145, Test Acc0.2955145118733509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:04<00:00,  3.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID5 Train Loss0.03173454447338978, Train Acc0.9993364299933643\n",
            "ID5 Val Loss1.9947250485420227, Val Acc0.5663600525624178\n",
            "ID5 Test Loss2.5776039361953735, Test Acc0.5045992115637319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:03<00:00,  3.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID6 Train Loss0.033676002795497574, Train Acc0.9996694214876033\n",
            "ID6 Val Loss4.305452108383179, Val Acc0.19866666666666666\n",
            "ID6 Test Loss0.7733030617237091, Test Acc0.783180026281209\n",
            "Train [0.59994637 0.8808739  0.8808739  0.8758912  0.8808739  0.88285743\n",
            " 0.8808739  0.88096846]\n",
            "Val [3.15515043 0.38992713 0.38992713 0.36169724 0.38992713 0.39856902\n",
            " 0.38992713 0.3910463 ]\n",
            "Test [3.3881712  0.37482056 0.37482056 0.34902759 0.37482056 0.38410254\n",
            " 0.37482056 0.37496429]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leave one trial out LSTM"
      ],
      "metadata": {
        "id": "dFms6R6clqEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "ID_list = [1,2,3,4,5,6]\n",
        "model_name_list = ['mlp', 'SVC']\n",
        "save_number = 4\n",
        "\n",
        "for m_idx in range(2):\n",
        "  model_list = []\n",
        "  for ID in range(1, 7):\n",
        "    model_name = result_path_isolated + '/'+ model_name_list[m_idx] + '_sub00' + str(ID) + '.pck'\n",
        "    model_stroke = pickle.load(open(model_name, \"rb\"))\n",
        "    model_list.append(model_stroke)\n",
        "\n",
        "  score = np.zeros((4, 3, 8))\n",
        "  for fold_idx in range(1, 5):\n",
        "    test_dataset = MyDataset(ID_list, R_time_list=[fold_idx], model_list=model_list, flatten=True, model_type=0)\n",
        "    test_loader = Data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    x = fold_idx+1 if fold_idx+1<=5 else 1\n",
        "    val_dataset = MyDataset(ID_list, R_time_list=[x], model_list=model_list, flatten=True, model_type=0)\n",
        "    val_loader = Data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    R_time_list = []\n",
        "    for i in range(1, 9):\n",
        "      if i!=fold_idx and i!=x:\n",
        "        R_time_list.append(i)\n",
        "    train_dataset = MyDataset(ID_list, R_time_list=R_time_list, model_list=model_list, flatten=True, model_type=0)\n",
        "    train_loader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "      \n",
        "    model_param=64\n",
        "    score[fold_idx-1] = main_function(total_epoch=300, dataloader_list=[train_loader, val_loader, test_loader], \\\n",
        "                                  model_para=model_param, model_save=False, cross_val=True, weight_decay=1e-4)\n",
        "    print('fold{} Train Loss{}, Train Acc{}'.format(fold_idx, score[fold_idx-1, 0, 0], score[fold_idx-1, 0, 1]))\n",
        "    print('fold{} Val Loss{}, Val Acc{}'.format(fold_idx, score[fold_idx-1, 1, 0], score[fold_idx-1, 1, 1]))\n",
        "    print('fold{} Test Loss{}, Test Acc{}'.format(fold_idx, score[fold_idx-1, 2, 0], score[fold_idx-1, 2, 1]))\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  np.save(result_path + '/d0420_score_' + str(save_number), score)\n",
        "  save_number += 1\n",
        "\n",
        "  x = np.mean(score, axis=0)\n",
        "  print('Train', x[0])\n",
        "  print('Val', x[1])\n",
        "  print('Test', x[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eXyYvkcloo3",
        "outputId": "d3506323-4602-4694-b70a-f9b1177d3e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "100%|██████████| 300/300 [01:57<00:00,  2.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold1 Train Loss0.00413815321570093, Train Acc1.0\n",
            "fold1 Val Loss0.15441490150988102, Val Acc0.9566666666666667\n",
            "fold1 Test Loss0.14860972340102307, Test Acc0.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "100%|██████████| 300/300 [02:00<00:00,  2.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold2 Train Loss0.0036076848098838873, Train Acc1.0\n",
            "fold2 Val Loss0.15129810012876987, Val Acc0.96\n",
            "fold2 Test Loss0.14222064922796562, Test Acc0.9555555555555556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "100%|██████████| 300/300 [02:00<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold3 Train Loss0.003998553369786929, Train Acc1.0\n",
            "fold3 Val Loss0.17663547277334146, Val Acc0.9454949944382648\n",
            "fold3 Test Loss0.18478432786650956, Test Acc0.9577777777777777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "100%|██████████| 300/300 [02:03<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold4 Train Loss0.004192808187905361, Train Acc1.0\n",
            "fold4 Val Loss0.14835733361542225, Val Acc0.9610244988864143\n",
            "fold4 Test Loss0.16630832944065332, Test Acc0.946607341490545\n",
            "Train [0.0039843 1.        1.        1.        1.        1.        1.\n",
            " 1.       ]\n",
            "Val [0.15767645 0.95579654 0.95579654 0.95533866 0.95579654 0.96078968\n",
            " 0.95579654 0.95577778]\n",
            "Test [0.16048076 0.95248517 0.95248517 0.95214025 0.95248517 0.95920635\n",
            " 0.95248517 0.9525    ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "100%|██████████| 300/300 [02:00<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold1 Train Loss0.0050264804370023985, Train Acc1.0\n",
            "fold1 Val Loss0.14951391017530113, Val Acc0.9611111111111111\n",
            "fold1 Test Loss0.1292533433297649, Test Acc0.9566666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "100%|██████████| 300/300 [02:01<00:00,  2.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold2 Train Loss0.004541903649541465, Train Acc1.0\n",
            "fold2 Val Loss0.20734457485377789, Val Acc0.9477777777777778\n",
            "fold2 Test Loss0.15044284294708632, Test Acc0.9511111111111111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "100%|██████████| 300/300 [02:02<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold3 Train Loss0.005345547635277564, Train Acc1.0\n",
            "fold3 Val Loss0.1699959586840123, Val Acc0.9377085650723026\n",
            "fold3 Test Loss0.1969612711109221, Test Acc0.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "100%|██████████| 300/300 [02:04<00:00,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold4 Train Loss0.004829184850677848, Train Acc1.0\n",
            "fold4 Val Loss0.13902956241508946, Val Acc0.9543429844097996\n",
            "fold4 Test Loss0.16824372136034071, Test Acc0.9454949944382648\n",
            "Train [0.00493578 1.         1.         1.         1.         1.\n",
            " 1.         1.        ]\n",
            "Val [0.166471   0.95023511 0.95023511 0.9498838  0.95023511 0.95663294\n",
            " 0.95023511 0.95022222]\n",
            "Test [0.16122529 0.95081819 0.95081819 0.95066475 0.95081819 0.95740079\n",
            " 0.95081819 0.95083333]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "INdrL3EhQ0wf",
        "ZtG9GXcaW3bo",
        "KztcCgr3jFtP",
        "lHhV-NanphiN",
        "fHWYiUTVdm3X",
        "V9OhRXC9drf1"
      ],
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
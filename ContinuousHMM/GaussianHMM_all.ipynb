{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OtYi5RtKQrz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pickle\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import balanced_accuracy_score,accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix,roc_auc_score,ConfusionMatrixDisplay,precision_score,recall_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkKOjbJwKTyT",
        "outputId": "fcc91c18-e9ca-4d47-ae56-c98d87ec7f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_dir = 'drive/MyDrive/23Spring-MLMA/continuous1/'\n",
        "os.listdir(main_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAMd0H1dK3BD",
        "outputId": "13c4c101-709a-4140-e8ef-066fd09d2b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ReadMe',\n",
              " '150word_label.xlsx',\n",
              " '50character_label.xlsx',\n",
              " 'common',\n",
              " '004',\n",
              " '001',\n",
              " '006',\n",
              " '005',\n",
              " '003',\n",
              " '002']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patient = [\"001\",\"002\",\"003\",\"004\",\"005\",\"006\"]\n",
        "test_split = [[\"01\",\"02\"],[\"03\",\"04\"],[\"05\",\"06\"],[\"07\",\"08\"],[\"09\",\"10\"]]\n",
        "val_split = ['03','01',\"04\",\"05\",\"06\"]"
      ],
      "metadata": {
        "id": "JiC0CJ1ZIhNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "df = pd.read_excel(main_dir+'150word_label.xlsx')\n",
        "number = list(df.iloc[:,0])\n",
        "sequence = list(df.iloc[:,2])\n",
        "sequence = [ast.literal_eval(string) for string in sequence]"
      ],
      "metadata": {
        "id": "RtzCwSF6v49R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"preprocess.ipynb\n",
        "Automatically generated by Colaboratory.\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1_HGxPyUOCJ3wnMfPbXl7sqWSz-jsCMGe\n",
        "\"\"\"\n",
        "\n",
        "# input: data (type: numpy array)(shape: (time, 2))\n",
        "# output: data (type: numpy array)(shape: (300, 2))\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "\n",
        "def preprocess(data):\n",
        "  # Delete silent part\n",
        "  data = data.astype(float)\n",
        "  n_size = 50\n",
        "  n_len = int(data.shape[0]/n_size)\n",
        "  std_data = np.zeros((n_size, 2))\n",
        "  for i in range(n_size):\n",
        "    seg_data_x = data[i*n_len:i*n_len+n_len, 0]\n",
        "    seg_data_y = data[i*n_len:i*n_len+n_len, 1]\n",
        "    std_data[i, 0] = np.std(seg_data_x)\n",
        "    std_data[i, 1] = np.std(seg_data_y)\n",
        "  pass_threshold = 1\n",
        "  pass_idx_x = np.where(std_data[:,0] >= pass_threshold)[0]\n",
        "  pass_idx_y = np.where(std_data[:,1] >= pass_threshold)[0]\n",
        "  if len(pass_idx_x) == 0:\n",
        "    start_idx = max(0, pass_idx_y[0] - 1)\n",
        "    end_idx = min(data.shape[0],pass_idx_y[-1] + 1)\n",
        "  elif len(pass_idx_y) == 0:\n",
        "    start_idx = max(0, pass_idx_x[0] - 1)\n",
        "    end_idx = min(data.shape[0],pass_idx_x[-1] + 1)\n",
        "  else:\n",
        "    start_idx = max(0,min(pass_idx_x[0], pass_idx_y[0]) - 1)\n",
        "    end_idx = min(data.shape[0],max(pass_idx_x[-1], pass_idx_y[-1]) + 1)\n",
        "  \n",
        "  # resample to 300 data points\n",
        "  data = signal.resample(data[start_idx*n_len:end_idx*n_len, :], 300, axis=0)\n",
        "  # scale\n",
        "  data = (data - data.min(axis=0, keepdims=True))/(data.max(axis=0, keepdims=True) - data.min(axis=0, keepdims=True))\n",
        "  # data = np.round(data)\n",
        "  # data = (data - data.min(axis=0, keepdims=True))\n",
        "  # data = data.astype(int)\n",
        "  return data\n",
        "\n",
        "\n",
        "# get file list for 1 patient\n",
        "def get_file_list(path):\n",
        "    file_list = []\n",
        "    label = []\n",
        "    sequence_l = []\n",
        "    print(path)\n",
        "    for i in os.listdir(path):\n",
        "        file_list.append(i)\n",
        "        l = int(i.split(\"_\")[-1].split(\".\")[0])-1\n",
        "        label.append(l)\n",
        "        sequence_l.append(sequence[l])\n",
        "    print(len(label))\n",
        "    return label,file_list\n",
        "    # return sequence_l,file_list\n",
        "\n",
        "# get feature from 1 file and preprocess\n",
        "def get_feature(path):\n",
        "    f = []\n",
        "    a = pd.read_csv(path,names=[\"vertical\",\"horizontal\"])\n",
        "    a = np.array(a)\n",
        "    a = preprocess(a)\n",
        "    for j in a[:,0]:\n",
        "        f.append(j)        \n",
        "    for j in a[:,1]:\n",
        "        f.append(j)\n",
        "    return f\n",
        "\n",
        "def evaluate(y_true, y_pred):\n",
        "    f1_micro = f1_score(y_true, y_pred,average = 'micro')\n",
        "    f1_macro = f1_score(y_true, y_pred,average = 'macro')\n",
        "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
        "    precision_macro = precision_score(y_true, y_pred, average='macro')\n",
        "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
        "    recall_macro = recall_score(y_true, y_pred, average='macro')\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    return f1_micro,f1_macro,precision_micro,precision_macro,recall_micro,recall_macro,acc"
      ],
      "metadata": {
        "id": "d3x7xg0y-kxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nf9tjR4HIkMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split on patient level - User-independent\n",
        "\n",
        "\n",
        "> 6-fold cross-validation\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-btOaAyBLRIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# self identified test_split\n",
        "def my_train_test_split_user_independent(test_patient,val_patient,train_patient):\n",
        "    X_test = []\n",
        "    X_train = []\n",
        "    X_val = []\n",
        "    y_val = []\n",
        "    y_train = []\n",
        "    y_test = []\n",
        "    for p in train_patient:\n",
        "        path = str(main_dir+p+\"/training/\")#str(\"../../../../data/isolated 2/\"+p+\"/isolated_strokes/\")\n",
        "        label,file_list = get_file_list(path)\n",
        "        for i in range(len(file_list)):\n",
        "            file = file_list[i]\n",
        "            file_label = label[i]\n",
        "            feature = get_feature(str(path+file))\n",
        "            X_train.append(feature)\n",
        "            y_train.append(file_label)\n",
        "        \n",
        "    path = str(main_dir+test_patient+\"/training/\")#str(\"../../../../data/isolated 2/\"+test_patient+\"/isolated_strokes/\")\n",
        "    label,file_list = get_file_list(path)\n",
        "    for i in range(len(file_list)):\n",
        "        file = file_list[i]\n",
        "        file_label = label[i]\n",
        "        feature = get_feature(str(path+file))\n",
        "        X_test.append(feature)\n",
        "        y_test.append(file_label)\n",
        "    \n",
        "    path = str(main_dir+val_patient+\"/training/\")# str(\"../../../../data/isolated 2/\"+val_patient+\"/isolated_strokes/\")\n",
        "    label,file_list = get_file_list(path)\n",
        "    for i in range(len(file_list)):\n",
        "        file = file_list[i]\n",
        "        file_label = label[i]\n",
        "        feature = get_feature(str(path+file))\n",
        "        X_val.append(feature)\n",
        "        y_val.append(file_label)\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_train = np.array(y_train)\n",
        "    y_test = np.array(y_test)\n",
        "    X_val = np.array(X_val)\n",
        "    y_val = np.array(y_val)\n",
        "    return X_train,X_test, X_val,y_val,y_train,y_test"
      ],
      "metadata": {
        "id": "1zvCw57RrOfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self identified test_split\n",
        "def my_train_test_split_user_independent(test_patient,val_patient,train_patient):\n",
        "  X_test = dict({})\n",
        "  X_train = dict({})\n",
        "  X_val = dict({})\n",
        "  for i in range(150):\n",
        "    X_test[i] = []\n",
        "    X_train[i] = []\n",
        "    X_val[i] = []\n",
        "  for p in train_patient:\n",
        "    path = str(main_dir+p+\"/training/\")\n",
        "    label,file_list = get_file_list(path)\n",
        "    for i in range(len(file_list)):\n",
        "      file_p = file_list[i]\n",
        "      file_label = label[i]\n",
        "      feature = get_feature(str(path+file_p))\n",
        "      if file_p.split('_')[2] in [\"01\", \"02\",\"03\",\"04\",\"05\"]:#,\"06\",\"07\",\"08\",\"09\",\"10\"]:\n",
        "        X_train[file_label].append(feature)\n",
        "        \n",
        "  path = str(main_dir+test_patient+\"/training/\")\n",
        "  label,file_list = get_file_list(path)\n",
        "  for i in range(len(file_list)):\n",
        "    file_p = file_list[i]\n",
        "    file_label = label[i]\n",
        "    feature = get_feature(str(path+file_p))\n",
        "    if file_p.split('_')[2] in [\"01\", \"02\",\"03\",\"04\",\"05\"]:#,\"06\",\"07\",\"08\",\"09\",\"10\"]:\n",
        "      X_test[file_label].append(feature)\n",
        "    \n",
        "  path = str(main_dir+val_patient+\"/training/\")\n",
        "  label,file_list = get_file_list(path)\n",
        "  for i in range(len(file_list)):\n",
        "    file_p = file_list[i]\n",
        "    file_label = label[i]\n",
        "    feature = get_feature(str(path+file_p))\n",
        "    if file_p.split('_')[2] in [\"01\", \"02\",\"03\",\"04\",\"05\"]:#,\"06\",\"07\",\"08\",\"09\",\"10\"]:\n",
        "      X_val[file_label].append(feature)\n",
        "\n",
        "  return X_train,X_test, X_val"
      ],
      "metadata": {
        "id": "3mn2bzPaCBh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hmmlearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-i-BE9Dzetl",
        "outputId": "690391ab-49d4-49d2-f6b2-e015208a6557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.5/160.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.9/dist-packages (from hmmlearn) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.9/dist-packages (from hmmlearn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.9/dist-packages (from hmmlearn) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.2.0)\n",
            "Installing collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hmmlearn import hmm\n",
        "start_probability = np.array([0.52,0.48])\n",
        "transition_probability = np.array([\n",
        "  [0.55, 0.45],\n",
        "  [0.45, 0.55]\n",
        "])\n",
        "emission_probability = np.array([\n",
        "  [0.083, 0.085, 0.086, 0.089, 0.086, 0.084, 0.081, 0.082, 0.071, 0.081, 0.087, 0.085],\n",
        "  [0.087, 0.083, 0.079, 0.088, 0.085, 0.078, 0.089, 0.08, 0.084, 0.083, 0.079, 0.085]\n",
        "])\n",
        "def get_stroke_hmms(n_states):\n",
        "  stroke_hmms = dict()\n",
        "  for i in range(12):\n",
        "    model = hmm.MultinomialHMM(n_components=n_states, n_iter=20, tol=0.001, random_state=42)\n",
        "    model.startprob_=start_probability\n",
        "    model.transmat_=transition_probability\n",
        "    model.emissionprob_=emission_probability\n",
        "    stroke_hmms[i] = model\n",
        "  return stroke_hmms\n",
        "  \n",
        "def get_word_hmms(n_states,stroke_hmms,sequence):\n",
        "  num_states = n_states*len(sequence)\n",
        "  word_transitions = np.zeros((num_states, num_states), dtype=float)\n",
        "  word_emissions = np.zeros((12, num_states, num_states), dtype=float)\n",
        "  model = hmm.MultinomialHMM(n_components=num_states, n_iter=20, tol=0.001, random_state=42)\n",
        "  pointer = 0\n",
        "  for i in range(len(sequence)):\n",
        "    current_stroke = sequence[i]\n",
        "    current_model = stroke_hmms[current_stroke]\n",
        "    model = hmm.MultinomialHMM(n_components=n_states)\n",
        "    start_probability = model.startprob_\n",
        "    transition_probability = model.transmat_\n",
        "    emission_probability = model.emissionprob_\n",
        "\n",
        "    word_transitions[pointer: pointer + n_states, pointer:pointer + n_states] = transition_probability\n",
        "    word_emissions[:, pointer: pointer + n_states, pointer:pointer + n_states] = emission_probability\n",
        "    pointer += n_states\n",
        "\n",
        "  model.transmat_=word_transitions\n",
        "  model.emissionprob_=word_emissions\n",
        "  return model"
      ],
      "metadata": {
        "id": "kp7asS__rrql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.min(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHD0XDZnBMpK",
        "outputId": "09b1b7bb-558b-47c3-f96a-12b3120fc6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Upclmw6bC0l5",
        "outputId": "fef41b77-5e0d-4d43-c21f-13ed358a3221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izxZcpQUBYtZ",
        "outputId": "cff1bad2-a6e9-42c9-9e48-8ff414333d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_patien_list = [\"001\",\"002\",\"003\",\"004\",\"005\",\"006\"]\n",
        "val_patient_list = [\"002\",\"003\",\"004\",\"005\",\"006\",\"001\"]\n",
        "output_pck = []\n",
        "F1 = 0\n",
        "N = [2,3]\n",
        "output = dict()\n",
        "\n",
        "val_pred = []\n",
        "val_label = []\n",
        "test_pred = []\n",
        "test_label = []\n",
        "\n",
        "for t in range(len(test_patien_list)):\n",
        "    train_patient= [\"001\",\"002\",\"003\",\"004\",\"005\",\"006\"]\n",
        "    test_patient = test_patien_list[t]\n",
        "    val_patient = val_patient_list[t]\n",
        "    train_patient.remove(test_patient)\n",
        "    train_patient.remove(val_patient)\n",
        "    print(test_patient,val_patient,train_patient)\n",
        "    # X_train,X_test, X_val,y_val,y_train,y_test = my_train_test_split_user_independent(test_patient,val_patient,train_patient)\n",
        "    X_train,X_test, X_val = my_train_test_split_user_independent(test_patient,val_patient,train_patient)\n",
        "    print(\"hey\")\n",
        "    clf_list = []\n",
        "    for k in range(150):\n",
        "      X_train_stroke = np.array(X_train[k])\n",
        "      clf = hmm.GaussianHMM(n_components=3, random_state=42)\n",
        "      clf.fit(X_train_stroke)\n",
        "      clf_list.append(clf)\n",
        "    pred = np.zeros((3,150))\n",
        "    for i in range(150):\n",
        "      score = np.zeros((3,150))\n",
        "      for j in range(150):\n",
        "        X_val_stroke = np.array(X_val[i])\n",
        "        X_test_stroke = np.array(X_test[i])\n",
        "        clf = clf_list[j]\n",
        "        score[0,j] = clf.score(X_val_stroke)\n",
        "        score[1,j] = clf.score(X_test_stroke[0].reshape(1, -1))\n",
        "        score[2,j] = clf.score(X_test_stroke[1].reshape(1, -1))\n",
        "      pred[:,i] = np.argmax(score,axis=1)\n",
        "    val_pred.extend(list(pred[0,:]))\n",
        "    val_label.extend(list(np.arange(150)))\n",
        "    test_pred.extend(list(pred[1,:]))\n",
        "    test_pred.extend(list(pred[2,:]))\n",
        "    test_label.extend(list(np.arange(150)))\n",
        "    test_label.extend(list(np.arange(150)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HFnvTLQDv_f",
        "outputId": "d0a0b525-d830-46a5-b6fe-7cab75dfd5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "001 002 ['003', '004', '005', '006']\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/003/training/\n",
            "756\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/004/training/\n",
            "758\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/005/training/\n",
            "761\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/006/training/\n",
            "761\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/001/training/\n",
            "750\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/002/training/\n",
            "750\n",
            "hey\n",
            "002 003 ['001', '004', '005', '006']\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/001/training/\n",
            "750\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/004/training/\n",
            "758\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/005/training/\n",
            "761\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/006/training/\n",
            "761\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/002/training/\n",
            "750\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/003/training/\n",
            "756\n",
            "hey\n",
            "003 004 ['001', '002', '005', '006']\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/001/training/\n",
            "750\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/002/training/\n",
            "750\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/005/training/\n",
            "761\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/006/training/\n",
            "761\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/003/training/\n",
            "756\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/004/training/\n",
            "758\n",
            "hey\n",
            "004 005 ['001', '002', '003', '006']\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/001/training/\n",
            "750\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/002/training/\n",
            "750\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/003/training/\n",
            "756\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/006/training/\n",
            "761\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/004/training/\n",
            "758\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/005/training/\n",
            "761\n",
            "hey\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.base:Model is not converging.  Current: 4586.160088769493 is not greater than 4586.160686704311. Delta is -0.0005979348179607769\n",
            "WARNING:hmmlearn.base:Model is not converging.  Current: 5083.432333126945 is not greater than 5083.432333143472. Delta is -1.652642822591588e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "005 006 ['001', '002', '003', '004']\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/001/training/\n",
            "750\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/002/training/\n",
            "750\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/003/training/\n",
            "756\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/004/training/\n",
            "758\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/005/training/\n",
            "761\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/006/training/\n",
            "761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.base:Model is not converging.  Current: 5483.075718173698 is not greater than 5483.075740702021. Delta is -2.2528322915604804e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey\n",
            "006 001 ['002', '003', '004', '005']\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/002/training/\n",
            "750\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/003/training/\n",
            "756\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/004/training/\n",
            "758\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/005/training/\n",
            "761\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/006/training/\n",
            "761\n",
            "drive/MyDrive/23Spring-MLMA/continuous1/001/training/\n",
            "750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.base:Model is not converging.  Current: 7191.023612630058 is not greater than 7191.023743660575. Delta is -0.00013103051696816692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.base:Model is not converging.  Current: 7515.075543719554 is not greater than 7515.155963670634. Delta is -0.08041995108033007\n",
            "WARNING:hmmlearn.base:Model is not converging.  Current: 6632.198294221661 is not greater than 6632.198294447009. Delta is -2.2534823074238375e-07\n",
            "WARNING:hmmlearn.base:Model is not converging.  Current: 6722.5981468468 is not greater than 6722.59815069191. Delta is -3.845109858957585e-06\n",
            "WARNING:hmmlearn.base:Model is not converging.  Current: 5607.580785736113 is not greater than 5607.580785796225. Delta is -6.011214281897992e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb3I81t7I2zR",
        "outputId": "9447164b-2511-43c7-9c32-9f7552459a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "900"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZprrs5WI4LQ",
        "outputId": "a04d8492-fdeb-4773-8d21-ac6dc8de96c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45.0"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_f1_micro,val_f1_macro,val_precision_micro,val_precision_macro,val_recall_micro,val_recall_macro,val_acc = evaluate(val_label,val_pred)\n",
        "test_f1_micro,test_f1_macro,test_precision_micro,test_precision_macro,test_recall_micro,test_recall_macro,test_acc = evaluate(test_label,test_pred)\n",
        "print(\"validation f1 micro:\", val_f1_micro)\n",
        "print(\"validation f1 macro:\", val_f1_macro)\n",
        "print(\"validation precision micro\", val_precision_micro)\n",
        "print(\"validation precision macro\", val_precision_macro)\n",
        "print(\"validation recall micro\", val_recall_micro)\n",
        "print(\"validation recall macro\", val_recall_macro)\n",
        "print(\"validation accuracy\", val_acc)\n",
        "print(\"test f1 micro:\", test_f1_micro)\n",
        "print(\"test f1 macro:\", test_f1_macro)\n",
        "print(\"test precision micro\", test_precision_micro)\n",
        "print(\"test precision macro\", test_precision_macro)\n",
        "print(\"test recall micro\", test_recall_micro)\n",
        "print(\"test recall macro\", test_recall_macro)\n",
        "print(\"test accuracy\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPurf5csIC3j",
        "outputId": "a47d8ca0-8924-4380-c21f-216aff333dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation f1 micro: 0.04777777777777778\n",
            "validation f1 macro: 0.04288299831723619\n",
            "validation precision micro 0.04777777777777778\n",
            "validation precision macro 0.06852352706074653\n",
            "validation recall micro 0.04777777777777778\n",
            "validation recall macro 0.04777777777777777\n",
            "validation accuracy 0.04777777777777778\n",
            "test f1 micro: 0.05388888888888889\n",
            "test f1 macro: 0.04978889601755972\n",
            "test precision micro 0.05388888888888889\n",
            "test precision macro 0.09833876518179271\n",
            "test recall micro 0.05388888888888889\n",
            "test recall macro 0.05388888888888888\n",
            "test accuracy 0.05388888888888889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}